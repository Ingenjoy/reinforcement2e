{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Rent Car World\n",
      "Policy evaluation (0)\n",
      "loop 1 : delta 191.1404, theta 0.0001\n",
      "loop 2 : delta 131.9191, theta 0.0001\n",
      "loop 3 : delta 88.6194, theta 0.0001\n",
      "loop 4 : delta 66.2761, theta 0.0001\n",
      "loop 5 : delta 52.3040, theta 0.0001\n",
      "loop 6 : delta 40.5043, theta 0.0001\n",
      "loop 7 : delta 31.5718, theta 0.0001\n",
      "loop 8 : delta 25.0090, theta 0.0001\n",
      "loop 9 : delta 20.7762, theta 0.0001\n",
      "loop 10 : delta 17.3732, theta 0.0001\n",
      "loop 11 : delta 14.4891, theta 0.0001\n",
      "loop 12 : delta 12.0544, theta 0.0001\n",
      "loop 13 : delta 10.0059, theta 0.0001\n",
      "loop 14 : delta 8.2881, theta 0.0001\n",
      "loop 15 : delta 6.8520, theta 0.0001\n",
      "loop 16 : delta 5.6550, theta 0.0001\n",
      "loop 17 : delta 4.6600, theta 0.0001\n",
      "loop 18 : delta 3.8349, theta 0.0001\n",
      "loop 19 : delta 3.1522, theta 0.0001\n",
      "loop 20 : delta 2.5884, theta 0.0001\n",
      "loop 21 : delta 2.1235, theta 0.0001\n",
      "loop 22 : delta 1.7407, theta 0.0001\n",
      "loop 23 : delta 1.4260, theta 0.0001\n",
      "loop 24 : delta 1.1675, theta 0.0001\n",
      "loop 25 : delta 0.9553, theta 0.0001\n",
      "loop 26 : delta 0.7814, theta 0.0001\n",
      "loop 27 : delta 0.6388, theta 0.0001\n",
      "loop 28 : delta 0.5221, theta 0.0001\n",
      "loop 29 : delta 0.4266, theta 0.0001\n",
      "loop 30 : delta 0.3485, theta 0.0001\n",
      "loop 31 : delta 0.2846, theta 0.0001\n",
      "loop 32 : delta 0.2324, theta 0.0001\n",
      "loop 33 : delta 0.1897, theta 0.0001\n",
      "loop 34 : delta 0.1548, theta 0.0001\n",
      "loop 35 : delta 0.1264, theta 0.0001\n",
      "loop 36 : delta 0.1031, theta 0.0001\n",
      "loop 37 : delta 0.0841, theta 0.0001\n",
      "loop 38 : delta 0.0686, theta 0.0001\n",
      "loop 39 : delta 0.0560, theta 0.0001\n",
      "loop 40 : delta 0.0457, theta 0.0001\n",
      "loop 41 : delta 0.0373, theta 0.0001\n",
      "loop 42 : delta 0.0304, theta 0.0001\n",
      "loop 43 : delta 0.0248, theta 0.0001\n",
      "loop 44 : delta 0.0202, theta 0.0001\n",
      "loop 45 : delta 0.0165, theta 0.0001\n",
      "loop 46 : delta 0.0134, theta 0.0001\n",
      "loop 47 : delta 0.0110, theta 0.0001\n",
      "loop 48 : delta 0.0089, theta 0.0001\n",
      "loop 49 : delta 0.0073, theta 0.0001\n",
      "loop 50 : delta 0.0059, theta 0.0001\n",
      "loop 51 : delta 0.0048, theta 0.0001\n",
      "loop 52 : delta 0.0040, theta 0.0001\n",
      "loop 53 : delta 0.0032, theta 0.0001\n",
      "loop 54 : delta 0.0026, theta 0.0001\n",
      "loop 55 : delta 0.0021, theta 0.0001\n",
      "loop 56 : delta 0.0017, theta 0.0001\n",
      "loop 57 : delta 0.0014, theta 0.0001\n",
      "loop 58 : delta 0.0012, theta 0.0001\n",
      "loop 59 : delta 0.0009, theta 0.0001\n",
      "loop 60 : delta 0.0008, theta 0.0001\n",
      "loop 61 : delta 0.0006, theta 0.0001\n",
      "loop 62 : delta 0.0005, theta 0.0001\n",
      "loop 63 : delta 0.0004, theta 0.0001\n",
      "loop 64 : delta 0.0003, theta 0.0001\n",
      "loop 65 : delta 0.0003, theta 0.0001\n",
      "loop 66 : delta 0.0002, theta 0.0001\n",
      "loop 67 : delta 0.0002, theta 0.0001\n",
      "loop 68 : delta 0.0002, theta 0.0001\n",
      "loop 69 : delta 0.0001, theta 0.0001\n",
      "loop 70 : delta 0.0001, theta 0.0001\n",
      "loop 71 : delta 0.0001, theta 0.0001\n",
      "Policy Improvement (0)\n",
      "318 policies are changed\n",
      "Policy evaluation (1)\n",
      "loop 1 : delta 64.0134, theta 0.0001\n",
      "loop 2 : delta 4.4455, theta 0.0001\n",
      "loop 3 : delta 2.0034, theta 0.0001\n",
      "loop 4 : delta 1.5196, theta 0.0001\n",
      "loop 5 : delta 1.3122, theta 0.0001\n",
      "loop 6 : delta 1.0846, theta 0.0001\n",
      "loop 7 : delta 0.8858, theta 0.0001\n",
      "loop 8 : delta 0.7214, theta 0.0001\n",
      "loop 9 : delta 0.5871, theta 0.0001\n",
      "loop 10 : delta 0.4776, theta 0.0001\n",
      "loop 11 : delta 0.3885, theta 0.0001\n",
      "loop 12 : delta 0.3159, theta 0.0001\n",
      "loop 13 : delta 0.2569, theta 0.0001\n",
      "loop 14 : delta 0.2090, theta 0.0001\n",
      "loop 15 : delta 0.1699, theta 0.0001\n",
      "loop 16 : delta 0.1382, theta 0.0001\n",
      "loop 17 : delta 0.1124, theta 0.0001\n",
      "loop 18 : delta 0.0914, theta 0.0001\n",
      "loop 19 : delta 0.0743, theta 0.0001\n",
      "loop 20 : delta 0.0604, theta 0.0001\n",
      "loop 21 : delta 0.0491, theta 0.0001\n",
      "loop 22 : delta 0.0400, theta 0.0001\n",
      "loop 23 : delta 0.0325, theta 0.0001\n",
      "loop 24 : delta 0.0264, theta 0.0001\n",
      "loop 25 : delta 0.0215, theta 0.0001\n",
      "loop 26 : delta 0.0175, theta 0.0001\n",
      "loop 27 : delta 0.0142, theta 0.0001\n",
      "loop 28 : delta 0.0116, theta 0.0001\n",
      "loop 29 : delta 0.0094, theta 0.0001\n",
      "loop 30 : delta 0.0076, theta 0.0001\n",
      "loop 31 : delta 0.0062, theta 0.0001\n",
      "loop 32 : delta 0.0051, theta 0.0001\n",
      "loop 33 : delta 0.0041, theta 0.0001\n",
      "loop 34 : delta 0.0033, theta 0.0001\n",
      "loop 35 : delta 0.0027, theta 0.0001\n",
      "loop 36 : delta 0.0022, theta 0.0001\n",
      "loop 37 : delta 0.0018, theta 0.0001\n",
      "loop 38 : delta 0.0015, theta 0.0001\n",
      "loop 39 : delta 0.0012, theta 0.0001\n",
      "loop 40 : delta 0.0010, theta 0.0001\n",
      "loop 41 : delta 0.0008, theta 0.0001\n",
      "loop 42 : delta 0.0006, theta 0.0001\n",
      "loop 43 : delta 0.0005, theta 0.0001\n",
      "loop 44 : delta 0.0004, theta 0.0001\n",
      "loop 45 : delta 0.0003, theta 0.0001\n",
      "loop 46 : delta 0.0003, theta 0.0001\n",
      "loop 47 : delta 0.0002, theta 0.0001\n",
      "loop 48 : delta 0.0002, theta 0.0001\n",
      "loop 49 : delta 0.0002, theta 0.0001\n",
      "loop 50 : delta 0.0001, theta 0.0001\n",
      "loop 51 : delta 0.0001, theta 0.0001\n",
      "Policy Improvement (1)\n",
      "260 policies are changed\n",
      "Policy evaluation (2)\n",
      "loop 1 : delta 4.2017, theta 0.0001\n",
      "loop 2 : delta 2.7978, theta 0.0001\n",
      "loop 3 : delta 1.8926, theta 0.0001\n",
      "loop 4 : delta 1.3512, theta 0.0001\n",
      "loop 5 : delta 0.9218, theta 0.0001\n",
      "loop 6 : delta 0.6108, theta 0.0001\n",
      "loop 7 : delta 0.4029, theta 0.0001\n",
      "loop 8 : delta 0.2699, theta 0.0001\n",
      "loop 9 : delta 0.1940, theta 0.0001\n",
      "loop 10 : delta 0.1468, theta 0.0001\n",
      "loop 11 : delta 0.1142, theta 0.0001\n",
      "loop 12 : delta 0.0933, theta 0.0001\n",
      "loop 13 : delta 0.0761, theta 0.0001\n",
      "loop 14 : delta 0.0620, theta 0.0001\n",
      "loop 15 : delta 0.0505, theta 0.0001\n",
      "loop 16 : delta 0.0411, theta 0.0001\n",
      "loop 17 : delta 0.0335, theta 0.0001\n",
      "loop 18 : delta 0.0273, theta 0.0001\n",
      "loop 19 : delta 0.0222, theta 0.0001\n",
      "loop 20 : delta 0.0181, theta 0.0001\n",
      "loop 21 : delta 0.0147, theta 0.0001\n",
      "loop 22 : delta 0.0120, theta 0.0001\n",
      "loop 23 : delta 0.0097, theta 0.0001\n",
      "loop 24 : delta 0.0079, theta 0.0001\n",
      "loop 25 : delta 0.0064, theta 0.0001\n",
      "loop 26 : delta 0.0052, theta 0.0001\n",
      "loop 27 : delta 0.0043, theta 0.0001\n",
      "loop 28 : delta 0.0035, theta 0.0001\n",
      "loop 29 : delta 0.0028, theta 0.0001\n",
      "loop 30 : delta 0.0023, theta 0.0001\n",
      "loop 31 : delta 0.0019, theta 0.0001\n",
      "loop 32 : delta 0.0015, theta 0.0001\n",
      "loop 33 : delta 0.0012, theta 0.0001\n",
      "loop 34 : delta 0.0010, theta 0.0001\n",
      "loop 35 : delta 0.0008, theta 0.0001\n",
      "loop 36 : delta 0.0007, theta 0.0001\n",
      "loop 37 : delta 0.0005, theta 0.0001\n",
      "loop 38 : delta 0.0004, theta 0.0001\n",
      "loop 39 : delta 0.0004, theta 0.0001\n",
      "loop 40 : delta 0.0003, theta 0.0001\n",
      "loop 41 : delta 0.0002, theta 0.0001\n",
      "loop 42 : delta 0.0002, theta 0.0001\n",
      "loop 43 : delta 0.0002, theta 0.0001\n",
      "loop 44 : delta 0.0001, theta 0.0001\n",
      "loop 45 : delta 0.0001, theta 0.0001\n",
      "loop 46 : delta 0.0001, theta 0.0001\n",
      "Policy Improvement (2)\n",
      "82 policies are changed\n",
      "Policy evaluation (3)\n",
      "loop 1 : delta 0.5303, theta 0.0001\n",
      "loop 2 : delta 0.1610, theta 0.0001\n",
      "loop 3 : delta 0.0753, theta 0.0001\n",
      "loop 4 : delta 0.0454, theta 0.0001\n",
      "loop 5 : delta 0.0309, theta 0.0001\n",
      "loop 6 : delta 0.0205, theta 0.0001\n",
      "loop 7 : delta 0.0132, theta 0.0001\n",
      "loop 8 : delta 0.0085, theta 0.0001\n",
      "loop 9 : delta 0.0055, theta 0.0001\n",
      "loop 10 : delta 0.0037, theta 0.0001\n",
      "loop 11 : delta 0.0027, theta 0.0001\n",
      "loop 12 : delta 0.0021, theta 0.0001\n",
      "loop 13 : delta 0.0017, theta 0.0001\n",
      "loop 14 : delta 0.0014, theta 0.0001\n",
      "loop 15 : delta 0.0011, theta 0.0001\n",
      "loop 16 : delta 0.0009, theta 0.0001\n",
      "loop 17 : delta 0.0008, theta 0.0001\n",
      "loop 18 : delta 0.0006, theta 0.0001\n",
      "loop 19 : delta 0.0005, theta 0.0001\n",
      "loop 20 : delta 0.0004, theta 0.0001\n",
      "loop 21 : delta 0.0003, theta 0.0001\n",
      "loop 22 : delta 0.0003, theta 0.0001\n",
      "loop 23 : delta 0.0002, theta 0.0001\n",
      "loop 24 : delta 0.0002, theta 0.0001\n",
      "loop 25 : delta 0.0001, theta 0.0001\n",
      "loop 26 : delta 0.0001, theta 0.0001\n",
      "loop 27 : delta 0.0001, theta 0.0001\n",
      "Policy Improvement (3)\n",
      "10 policies are changed\n",
      "Policy evaluation (4)\n",
      "loop 1 : delta 0.0519, theta 0.0001\n",
      "loop 2 : delta 0.0074, theta 0.0001\n",
      "loop 3 : delta 0.0033, theta 0.0001\n",
      "loop 4 : delta 0.0019, theta 0.0001\n",
      "loop 5 : delta 0.0011, theta 0.0001\n",
      "loop 6 : delta 0.0006, theta 0.0001\n",
      "loop 7 : delta 0.0004, theta 0.0001\n",
      "loop 8 : delta 0.0002, theta 0.0001\n",
      "loop 9 : delta 0.0001, theta 0.0001\n",
      "loop 10 : delta 0.0001, theta 0.0001\n",
      "Policy Improvement (4)\n",
      "0 policies are changed\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Agent' object has no attribute 'v_lables'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0c5847ea6a70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 527\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    528\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"End Rent Car World\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-0c5847ea6a70>\u001b[0m in \u001b[0;36mpolicy_iteration\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;31m# 4. Visualization of Value Function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_lables\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Value Function %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miter_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflipud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_function\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"./images/figure_4_2_rentcar_%d.png\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0miter_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Agent' object has no attribute 'v_lables'"
     ]
    }
   ],
   "source": [
    "\"\"\" car_rental\n",
    "Chapter 4. Car Rental Example\n",
    "Author : SeongJin Yoon\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import math\n",
    "import itertools\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "MIN = 0\n",
    "MAX = 1\n",
    "\n",
    "class ConfigDict(dict):\n",
    "    def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs)\n",
    "    def __getattr__(self, name): return self[name]\n",
    "    def __setattr__(self, name, value): self[name] = value\n",
    "    def __delattr__(self, name): del self[name]\n",
    "        \n",
    "config = ConfigDict()\n",
    "config.env = ConfigDict(max_managed_cars = 20, max_movable_cars = 5, num_branch = 2)\n",
    "config.branch1 = ConfigDict(lambda_rent = 3, lambda_return = 3)\n",
    "config.branch2 = ConfigDict(lambda_rent = 4, lambda_return = 2)\n",
    "config.value = ConfigDict(discount = 0.9, theta = 1e-4)\n",
    "config.poisson = ConfigDict(upperbound = 11)\n",
    "config.plot = ConfigDict(save_dir = './sjyoon/')\n",
    "\n",
    "class Plot():\n",
    "    def __init__(self, nrows, ncols, num_ticks, coord_list):\n",
    "        self.fig_idx = 0\n",
    "\n",
    "        self.coord_list = coord_list\n",
    "        self.xticks = list(range(num_ticks+1))\n",
    "        self.yticks = list(reversed(range(num_ticks+1)))\n",
    "        \n",
    "        _, self.axes = plt.subplots(nrows=nrows, ncols=ncols,  figsize=(40, 20))\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "        self.axes = self.axes.flatten()\n",
    "\n",
    "    # plot a policy/state value matrix\n",
    "    def draw_heatmap(self, data, labels):\n",
    "        fig = sns.heatmap(data, cmap=\"YlGnBu\", ax=self.axes[self.fig_idx])\n",
    "        fig.set_xticks(self.xticks)\n",
    "        fig.set_yticks(self.yticks)\n",
    "        fig.set_ylabel(labels[0], fontsize=30)\n",
    "        fig.set_xlabel(labels[1], fontsize=30)\n",
    "        fig.set_title(labels[2], fontsize=30)\n",
    "        self.fig_idx += 1\n",
    "\n",
    "    def save(self, title):\n",
    "        plt.savefig(title)\n",
    "\n",
    "    def show(self):\n",
    "        plt.show()\n",
    "\n",
    "    def close(self):\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "class Poisson():\n",
    "    def __init__(self):\n",
    "        self.dist = dict()\n",
    "        \n",
    "    def prob(self, n, mean):\n",
    "        key = n * 10 + mean\n",
    "        if key not in self.dist.keys():\n",
    "            self.dist[key] = math.exp(-mean) * pow(mean, n) / math.factorial(n)\n",
    "\n",
    "        return self.dist[key]\n",
    "    \n",
    "    def __call__(self, n, mean):\n",
    "        return self.prob(n, mean)\n",
    "\n",
    "\n",
    "class Reward(Enum):\n",
    "    \"\"\"Reward classes.\"\"\"\n",
    "    rent = 1\n",
    "    move = 2\n",
    "\n",
    "class Transaction():\n",
    "    \"\"\"Request and Return transaction classes.\"\"\"\n",
    "    def __init__(self, dist, mean):\n",
    "        self.dist = dist\n",
    "        self.mean = mean\n",
    "\n",
    "    def prob(self, num_tx):\n",
    "        return self.dist.prob(num_tx, self.mean)\n",
    "    \n",
    "class Environment():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.prepare_environment()\n",
    "        \n",
    "        self.branch1 = Branch(config.branch1)\n",
    "        self.branch2 = Branch(config.branch2)\n",
    "        self.branch_list = [self.branch1, self.branch2]\n",
    "        \n",
    "        poisson_dist = Poisson()\n",
    "        self.request1 = Transaction(poisson_dist, config.branch1.lambda_rent)\n",
    "        self.request2 = Transaction(poisson_dist, config.branch2.lambda_rent)\n",
    "        self.return1 = Transaction(poisson_dist, config.branch1.lambda_return)\n",
    "        self.return2 = Transaction(poisson_dist, config.branch2.lambda_return)\n",
    "        \n",
    "\n",
    "    def prepare_environment(self):\n",
    "        self.num_state = config.env.max_managed_cars + 1\n",
    "        self.num_action = config.env.max_movable_cars # max number of cars to be moved\n",
    "        self.num_request = config.poisson.upperbound\n",
    "        self.num_return = config.poisson.upperbound\n",
    "        \n",
    "        \n",
    "        self.state_list = self.make_pair_list(self.num_state)\n",
    "        self.action_list = list(range(0, self.num_action+1)) + list(range(-1, -(self.num_action+1), -1))\n",
    "        self.request_list = self.make_pair_list(self.num_request)\n",
    "        self.return_list = self.make_pair_list(self.num_return)\n",
    "        self.reward_list = {Reward.rent : 10, Reward.move : -2}\n",
    "        return\n",
    "    \n",
    "    def make_pair_list(self, num_pair):\n",
    "        return list(itertools.product(range(num_pair), repeat=2))\n",
    "    \n",
    "    def get_statelist(self):\n",
    "        return self.state_list\n",
    "    \n",
    "    def get_actionlist(self):\n",
    "        return self.action_list\n",
    "\n",
    "    def get_requestlist(self):\n",
    "        return self.request_list\n",
    "\n",
    "    def get_returnlist(self):\n",
    "        return self.return_list\n",
    "\n",
    "    def get_num_state(self):\n",
    "        return self.num_state\n",
    "\n",
    "    def get_num_action(self):\n",
    "        return self.num_action\n",
    "    \n",
    "    def get_state(self):\n",
    "        state = tuple((self.branch1.get_state(), self.branch2.get_state()))\n",
    "        return state\n",
    "\n",
    "    def get_reward(self, reward_type):\n",
    "        return self.reward_list[reward_type]\n",
    "\n",
    "    def rent_prob(self, requests):\n",
    "        r1, r2 = requests\n",
    "        return self.request1.prob(r1) * self.request2.prob(r2)\n",
    "            \n",
    "    def return_prob(self, returns):\n",
    "        r1, r2 = returns\n",
    "        return self.return1.prob(r1) * self.return2.prob(r2)\n",
    "\n",
    "    def validate(self, state, action):\n",
    "        self.reset_state(state)\n",
    "        \n",
    "        from_branch = self.branch1\n",
    "        to_branch = self.branch2\n",
    "        requests = action\n",
    "        \n",
    "        if action < 0:\n",
    "            from_branch = self.branch2\n",
    "            to_branch = self.branch1\n",
    "            requests = -action\n",
    "        \n",
    "        if from_branch.get_available_cars() < requests \\\n",
    "            or to_branch.get_acceptable_cars() < requests:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    def lookahead_do_action(self, state, action):\n",
    "        \"\"\" move cars from one branch to another branch\"\"\"\n",
    "\n",
    "        if self.validate(state, action)  is False:\n",
    "            return 0\n",
    "\n",
    "        self.reset_state(state)\n",
    "\n",
    "        from_branch = self.branch1\n",
    "        to_branch = self.branch2\n",
    "        requests = action\n",
    "        \n",
    "        if action < 0:\n",
    "            from_branch = self.branch2\n",
    "            to_branch = self.branch1\n",
    "            requests = -action\n",
    "       \n",
    "        cars_move_from = from_branch.move_from(requests)\n",
    "        cars_move_info = to_branch.move_into(requests)\n",
    "        \n",
    "        assert cars_move_from == cars_move_info\n",
    "\n",
    "        reward = requests*self.get_reward(Reward.move)\n",
    "        return self.get_state(), reward\n",
    "\n",
    "    def lookahead_rent_cars(self, state, requests):\n",
    "        self.reset_state(state)\n",
    "        \n",
    "        r1, r2 = requests\n",
    "        rent_cars1 = self.branch1.rent_cars(r1)\n",
    "        rent_cars2 = self.branch2.rent_cars(r2)\n",
    "       \n",
    "        total_rent_cars = rent_cars1 + rent_cars2\n",
    "        reward = total_rent_cars*self.get_reward(Reward.rent)\n",
    "        \n",
    "        return self.get_state(), reward\n",
    "\n",
    "    def lookahead_return_cars(self, state, returns):\n",
    "        self.reset_state(state)\n",
    "            \n",
    "        r1, r2 = returns\n",
    "        self.branch1.return_cars(r1)\n",
    "        self.branch2.return_cars(r2)\n",
    "        \n",
    "        return self.get_state()\n",
    "    \n",
    "    def reset_state(self, state):\n",
    "        s1, s2 = state\n",
    "        self.branch1.set_state(s1)\n",
    "        self.branch2.set_state(s2)\n",
    "\n",
    "class Branch():\n",
    "    def __init__(self, branch):\n",
    "        self.branch = branch\n",
    "        self.state_range = [0,config.env.max_managed_cars]\n",
    "        self.state = config.env.max_managed_cars\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "    \n",
    "    def set_state(self, state):\n",
    "        assert self.in_range(state)\n",
    "        self.state = state\n",
    "\n",
    "    def in_range(self, state):\n",
    "        if self.state_range[MIN] <= state \\\n",
    "            and state <= self.state_range[MAX]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_available_cars(self):\n",
    "        return self.state - self.state_range[MIN]\n",
    "    \n",
    "    def get_acceptable_cars(self):\n",
    "        return self.state_range[MAX] - self.state\n",
    "\n",
    "    def rent_cars(self, request_cars):\n",
    "        available_cars = self.state - self.state_range[MIN]\n",
    "        \n",
    "        if available_cars < request_cars:\n",
    "            rent_cars = available_cars\n",
    "        else:\n",
    "            rent_cars = request_cars\n",
    "        \n",
    "        self.state -= rent_cars\n",
    "        return rent_cars\n",
    "    \n",
    "    def return_cars(self, request_cars):\n",
    "        acceptable_cars = self.state_range[MAX] - self.state\n",
    "        \n",
    "        if acceptable_cars < request_cars:\n",
    "            return_cars = acceptable_cars\n",
    "        else:\n",
    "            return_cars = request_cars\n",
    "\n",
    "        self.state += return_cars\n",
    "        return return_cars\n",
    "\n",
    "    def move_from(self, request_cars):\n",
    "        if request_cars > config.env.max_movable_cars:\n",
    "            return 0\n",
    "        \n",
    "        available_cars = self.get_available_cars()\n",
    "        \n",
    "        if available_cars < request_cars:\n",
    "            move_cars = available_cars\n",
    "        else:\n",
    "            move_cars = request_cars\n",
    "        \n",
    "        self.state -= move_cars\n",
    "        return move_cars\n",
    "\n",
    "    def move_into(self, request_cars):\n",
    "        if request_cars > config.env.max_movable_cars:\n",
    "            return 0\n",
    "        \n",
    "        acceptable_cars = self.get_acceptable_cars()\n",
    "        \n",
    "        if acceptable_cars <= request_cars:\n",
    "            move_cars = acceptable_cars\n",
    "        else:\n",
    "            move_cars = request_cars\n",
    "\n",
    "        self.state += move_cars\n",
    "        return move_cars\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self, env):\n",
    "        \n",
    "        self.env = env\n",
    "        self.qurey_environment()\n",
    "        \n",
    "        self.plot = Plot(2, 3, self.num_state, self.state_list)\n",
    "        self.policy_labels = ['# of cars in branch 1', '# of cars in branch 2', 'Policy']\n",
    "        self.v_labels = ['# of cars in branch 1', '# of cars in branch 2', 'Value Function']\n",
    "\n",
    "\n",
    "    def qurey_environment(self):\n",
    "        self.num_state = self.env.get_num_state()\n",
    "        self.num_action = self.env.get_num_action()\n",
    "\n",
    "        self.state_list = self.env.get_statelist()\n",
    "        self.action_list = self.env.get_actionlist()\n",
    "        self.request_list = self.env.get_requestlist()\n",
    "        self.return_list = self.env.get_returnlist()\n",
    "        \n",
    "    \n",
    "    def policy_iteration(self):\n",
    "        \n",
    "        # 1. Initialize\n",
    "        self.policy = np.zeros((self.num_state, self.num_state), dtype=np.int8)\n",
    "        self.value_function = np.zeros((self.num_state, self.num_state))\n",
    "\n",
    "        iter_count = 0\n",
    "        policy_stable = False\n",
    "        while policy_stable is False:            \n",
    "            # 3. Visualization of Policy\n",
    "            self.policy_labels[2] = \"Policy %d\" % (iter_count)\n",
    "            self.plot.draw_heatmap(np.flipud(self.policy), self.policy_labels)\n",
    "            filename = config.plot.save_dir + \"figure_4_2_rentcar_%d.png\" % (iter_count)\n",
    "            self.plot.save(filename)\n",
    "\n",
    "            # 2. Policy Evaluation\n",
    "            print(\"Policy evaluation (%d)\" % (iter_count))\n",
    "            self.policy_evaluation()\n",
    "\n",
    "            # 3. Policy Improvement\n",
    "            print(\"Policy Improvement (%d)\" % (iter_count))\n",
    "            policy_stable = self.policy_improvement()\n",
    "\n",
    "            iter_count += 1\n",
    "\n",
    "        # 4. Visualization of Value Function\n",
    "        self.v_labels[2] = \"Value Function %d\" % (iter_count)\n",
    "        self.plot.draw_heatmap(np.flipud(self.value_function) , self.v_labels)\n",
    "        filename = config.plot.save_dir + \"figure_4_2_rentcar_final.png\"\n",
    "        self.plot.save(filename)       \n",
    "        self.plot.close()\n",
    "\n",
    "    def policy_evaluation(self):\n",
    "        \n",
    "        theta = config.value.theta\n",
    "        loop_count = 1\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state in self.state_list:\n",
    "                s1, s2 = state\n",
    "                v = self.value_function[s1, s2]\n",
    "                action_idx = self.policy[s1, s2]\n",
    "                new_value = self.calc_qvalue(state, action_idx)\n",
    "                self.value_function[s1, s2] = new_value\n",
    "                \n",
    "                delta = max(delta, math.fabs(v - new_value)) \n",
    "                \n",
    "            print(\"loop %d : delta %.4f, theta %.4f\" % (loop_count, delta, theta))\n",
    "            #print(self.V.get_array())\n",
    "            loop_count += 1\n",
    "            \n",
    "            if delta < theta:\n",
    "                break\n",
    "        \n",
    "    def policy_improvement(self):\n",
    "        \n",
    "        change_count = 0\n",
    "        old_policy_sum = np.sum(self.policy)\n",
    "        \n",
    "        for state in self.state_list:\n",
    "            s1, s2 = state\n",
    "            old_action = self.policy[s1, s2]\n",
    "\n",
    "            # calculate return for each action\n",
    "            qvalue_list = []\n",
    "            for action in self.action_list:\n",
    "                if self.env.validate(state, action):\n",
    "                    qvalue_list.append(self.calc_qvalue(state, action))\n",
    "                else:\n",
    "                    qvalue_list.append(-float('inf'))\n",
    "            \n",
    "            # pick baset action\n",
    "            best_action = self.action_list[self.pick_best(qvalue_list)]\n",
    "            self.policy[s1, s2] = best_action\n",
    "            \n",
    "            if old_action != best_action:\n",
    "                change_count += 1\n",
    "                \n",
    "        policy_stable = True\n",
    "        if old_policy_sum != np.sum(self.policy):\n",
    "            policy_stable = False\n",
    "\n",
    "        print(\"%d policies are changed\" % (change_count))            \n",
    "        #print(self.policy.get_array())             \n",
    "        return policy_stable\n",
    "    \n",
    "    def pick_best(self, candidates):\n",
    "        best_idx = np.argmax(candidates)\n",
    "        return best_idx\n",
    "    \n",
    "    def calc_qvalue(self, state, action):\n",
    "       \n",
    "        new_state, immediate_reward = self.env.lookahead_do_action(state, action)\n",
    "        discounted_return = self.lookahead_daily_transaction(new_state)\n",
    " \n",
    "        exptected_return = immediate_reward + discounted_return\n",
    "        return exptected_return\n",
    "    \n",
    "    def lookahead_daily_transaction(self, state):\n",
    "        discount = config.value.discount\n",
    "        discounted_return = 0\n",
    "        for requests in self.request_list:\n",
    "            new_state, reward = self.env.lookahead_rent_cars(state, requests)\n",
    "            prob_request = self.env.rent_prob(requests)\n",
    "\n",
    "            for returns in self.return_list:\n",
    "                new_state2 = self.env.lookahead_return_cars(new_state, returns)\n",
    "                prob_return = self.env.return_prob(returns)\n",
    "                prob = prob_request * prob_return\n",
    "                s1, s2 = new_state2\n",
    "                discounted_return += prob * \\\n",
    "                    (reward + discount * self.value_function[s1, s2] )\n",
    " \n",
    "        return discounted_return\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    print(\"Start Rent Car World\")\n",
    "    env = Environment()\n",
    "    agent = Agent(env)\n",
    "    agent.policy_iteration()\n",
    "    print(\"End Rent Car World\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLGosu",
   "language": "python",
   "name": "rlgosu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
