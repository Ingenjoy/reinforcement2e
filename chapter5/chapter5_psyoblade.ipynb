{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5. Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 이번 장에서는 가치 함수들을 추정하고, 최적 정책을 발견하기 위한 학습 기법들을 다룬다. 단, complete knowledge of environment 를 가정하지 않고 optimal behavior 를 얻을 것이다.\n",
    "> DP와 같이 모든 가능한 transition 값이 아니라 샘플링 된 transitions 값만으로도 충분히 강력한데, Monte Carlo methods 는 average sample returns 기반으로 강화 학습 문제들을 푼다.\n",
    "\n",
    " episotic task 들에 대해서만 다루며 하나의 episode 가 종료되었을 때에 value estimates 와 policies 값들이 변경된다. 즉, Monte Carlo methods 는 episode-by-episode 수준에서 점진적으로 학습이 이루어진다. (not step-by-step)\n",
    " \n",
    "  가장 큰 차이점은 multiple states 가 존재한다는 것이고, 반환 값은 같은 에피소드 내에 다음에 발생할 states 들에 의존한다. 즉, 모든 행동 선택이 학습 도중에 발생한다는 것이고 이전 state 관점에서 보았을 때에는 nonstationary 한 환경이다. 이러한 nonstationarity 는 DP를 위한 GPI 를 통해 샘플의 반환값을 통해 가치 함수를 학습한다. \n",
    "  \n",
    "   prediction problem -> policy improvement -> control problem using GPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Monte Carlo Prediction\n",
    " 경험을 통해서 추정하는 가장 명백한 방법은 state 의 방문 이후에 반환 값들의 평균값을 이용하는 것이여 이 아이디어는 모든 Monte Carlo methods 들의 기본이다.\n",
    " \n",
    " #### MC Methods\n",
    " ##### first-visit MC methods : state 에 처음 방문하는 경우들의 rewards 의 평균을 취함\n",
    " ##### every-visit MC methods : S의 이전 방문 경험에 대한 체크가 빠진 점만 차이점이다\n",
    "> 어느 방법이든 방문 회수가 무한대로 가는 경우 가치함수는 수렴하게  되어 있으며, 대수의 법칙에 따라 estimates 값들의 averages 값은 expected value 에 수렴한다.\n",
    "![first-visit](images/mc-first.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5.1: Blackjack\n",
    "<br>\n",
    "#### 기본적인 규칙\n",
    "##### 1. 21을 초과하지 않고 카드 숫자의 합이 가능한 카드 조합을 찾는 문제\n",
    "##### 2. 모든 얼굴 카드는 10으로 계산되며, 에이스는 1 또는 11로 계산 \n",
    "##### 3. 각 플레이어들이 딜러와 독립적으로 경쟁하는 게임\n",
    "##### 4. 게임은 딜러와 플레이어 모두에게 전달되는 두 장의 카드로 시작\n",
    "##### 5. 딜러의 카드 중 하나가 앞면이 보이고 다른 쪽이 뒤집혀 있습니다\n",
    "##### 6. 플레이어가 첫 번째 받은 2장이 A + 10 (10, J, Q, K) 이면 Blackjack or Natural 이라고 합니다.\n",
    "##### 6-1. 이 때에 딜러가 Natural 이라면 Draw, 아니면 플레이어가 승리합니다.\n",
    "##### 6-2. 반대의 경우 플레이어가 Natural 이 아니면, 순서대로 한 장식 더 가짐(Hit).\n",
    "##### 6-3. 숫자가 큰 경우 거부(Stick)할 수 있으며 21을 넘어서는 경우 Bust 가 됩니다.\n",
    "##### 7. Bust 경우 지게되고, 플레이어가 Stick 하면 딜러의 차례가 됩니다\n",
    "##### 7-1. 딜러는 17이상이면 Stick 하는 고정 전략을 따르고 그 이하는 Hit 합니다.\n",
    "##### 8. 딜러가 Bust 하면 플레이어가 이기고, 그 외에는 21에 가까운 경우가 승리합니다.\n",
    "<br>\n",
    "#### MDP 문제로의 이해\n",
    "##### 1. 블랙잭 게임은 episodic MDP 문제이며 매 경기는 하나의 episode 입니다\n",
    "##### 2. 보상은 게임 종료 시에 승리 +1, 패배 -1, 무승부 0, discount factor 는 없으므로 $\\gamma = 0$\n",
    "##### 3. 플레이어의 action 은 hit or stick\n",
    "##### 4. state 는 보여지는 플레이어의 카드와 딜러의 카드입니다\n",
    "##### 5. 카드는 이미 처리 된 카드를 추적 할 수있는 이점이 없도록하기 위해 카드를 교체합니다 (즉 교체 포함)\n",
    "##### 6. 플레이어는 세 가지 변수에 근거하여 결정을 내리며 총 200개의 상태가 만들어집니다.\n",
    "##### 6-1. 플레이어 카드의 현재 숫자의 합 (12-21) [10]\n",
    "##### 6-2. 딜러가 보여주는 한 장의 카드 (ace-10) [10]\n",
    "##### 6-3.  사용 가능한 ace 카드의 보유 여부 [2]\n",
    "> 플레이어 숫자의 합이 20 or 21 이라면 stick 아니면 hit 하는 정책을 생각해 보면,  Monte Carlo approach 를 통해 state-value function 을 찾기 위해 매 상태의 returns 의 averages 와 policy 를 이용하여 많은 blackjack 게임을 수행할 수 있다. 이를 통하여 estimates of state-value function 을 구할 수 있다.\n",
    "\n",
    "![Estimates for states](images/figure5.1.png)\n",
    "The estimates for states with a usable ace are less certain and less regular because these\n",
    "states are less common. In any event, after 500,000 games the value function is very well\n",
    "approximated.\n",
    "\n",
    " 블랙 잭 작업에서 환경에 대한 완전한 지식을 가지고 있지만 값 기능을 계산하기 위해 DP 방법을 적용하는 것은 쉽지 않으며, DP 방법은 다음 이벤트의 분포를 필요로합니다. 특히 4 가지 인수 함수 p가 제공하는 환경 dynamics 가 필요하며 블랙잭을 위해 이것을 결정하는 것은 쉽지 않습니다.\n",
    "> 플레이어의 합이 14이고 stick 을 선택한다고 가정할 때,  딜러의 카드 표시 기능에 +1의 보상으로 종료 할 확률은?\n",
    "\n",
    " DP가 적용되기 전에 모든 확률이 계산되어야하며 그러한 계산은 종종 복잡하고 오류가 발생하기 쉽습니다. 반대로 몬테카를로 메서드가 필요로하는 샘플 게임을 생성하는 것은 쉽습니다. Monte Carlo 방법은 에피소드 샘플링 만으로도 동작할 수 있는 능력은 환경의 dynamics 에 대한 완전한 지식을 갖추고있을 때조차도 큰 이점이 될 수 있습니다.\n",
    "> 백업 다이어그램에 대한 아이디어를 Monte Carlo 알고리즘으로 일반화 할 수 있습니까?\n",
    "\n",
    " 백업 다이어그램의 일반적인 개념은 업데이트 할 루트 노드를 맨 위에 표시하고 보상 및 예상 값이 업데이트에 기여하는 모든 전환 및 리프 노드를 표시하는 것입니다. v의 몬테카를로 추정에있어서, 루트는 상태 노드이고, 그 아래에는 특정 단일 에피소드를 따르는 전이 궤도의 전체가있다.\n",
    "<br>\n",
    "#### DP 와 MC 의 차이점\n",
    "![backup](images/backup.png)\n",
    "##### DP가 가능한 모든 전환을 표시, MC는 한 에피소드에서 샘플링 된 것만 보여줌\n",
    "##### DP가 하나의 단계 전환 만 포함, MC는 에피소드가 끝날 때까지 계속된다.\n",
    "\n",
    "<br>\n",
    "#### MC 가 DP 보다 좋은 점\n",
    "##### 1. MC 는 개별 state 에 대한 estimates 가  독립적이다.\n",
    "#####  - 하나의 state 에 대한 estimate는 DP와 같이 다른 state 에 대한 estimate 을 기초로하지 않음\n",
    "##### 2. MC는 bootstrap 하지 않으며,  단일 상태 값을 추정하는 계산 비용은 상태 수와 무관\n",
    "#####  - 특히 one state or sub-state 의 value 만 얻기에 유용함\n",
    "##### 3. 관심 state 에서 시작하여 많은 샘플 에피소드를 생성 할 수 있고, 다른 모든 것을 무시하고 이러한 상태에서만 평균을 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "deck = []\n",
    "player = []\n",
    "dealer = []\n",
    "episodes = {}\n",
    "\n",
    "# global static variables\n",
    "q_returns = {}\n",
    "p_returns = {}\n",
    "ace_result = {}\n",
    "\n",
    "def reset_global_variables():\n",
    "    global deck, player, dealer, episodes\n",
    "    deck = []\n",
    "    player = []\n",
    "    dealer = []\n",
    "    episodes = []\n",
    "\n",
    "def init_deck():\n",
    "    global deck\n",
    "    deck = [x for x in range(1, 11) for y in range(0, 4)] # 0~10 * 4\n",
    "    for x in range(0, 3 * 4): deck.append(10)\n",
    "\n",
    "def get_card():\n",
    "    global deck\n",
    "    low, high = 0, len(deck) - 1\n",
    "    x = random.randint(low, high)\n",
    "    return deck.pop(x)\n",
    "\n",
    "def shuffle(name, agent, num):\n",
    "    for x in range(0, num):\n",
    "        card = get_card()\n",
    "        agent.append(card)\n",
    "#     print(name, agent)\n",
    "\n",
    "# action occurred or not\n",
    "# agent.count 계산 후에 반환 값을 else 로 처리하지 않아 덮어쓰는 버그\n",
    "def play(name, agent, threshold):\n",
    "    global player, dealer\n",
    "    stick_or_hit = None\n",
    "    if agent.count(1) <= 0:\n",
    "        stick_or_hit = sum(agent) < threshold # 20, 21\n",
    "    else:\n",
    "        stick_or_hit = sum(agent) + 10 < threshold # over 17 ~ 21\n",
    "#     print(name, stick_or_hit)\n",
    "    return stick_or_hit\n",
    "\n",
    "def initiate_player_state():\n",
    "    global player\n",
    "    value = sum_of_cards(player)\n",
    "    while value < 12:\n",
    "        shuffle('player', player, 1)\n",
    "        value = sum_of_cards(player)\n",
    "\n",
    "def generate_episode(stick_or_hit):\n",
    "    global player, dealer, episodes\n",
    "    usable_ace_player = True if player.count(1) > 0 else False\n",
    "    player_sum = sum_of_cards(player)\n",
    "    dealer_card1 = dealer[0]\n",
    "    episodes.append((usable_ace_player, player_sum, dealer_card1, stick_or_hit))\n",
    "\n",
    "def random_generator():\n",
    "    global player, dealer\n",
    "    while play('player', player, 20):\n",
    "        generate_episode(True)\n",
    "        shuffle('player', player, 1)\n",
    "    generate_episode(False)\n",
    "    while play('dealer', dealer, 17):\n",
    "        shuffle('dealer', dealer, 1)\n",
    "\n",
    "def sum_of_cards(agent):\n",
    "    if agent.count(1) <= 0:\n",
    "        return sum(agent)\n",
    "    if sum(agent) + 10 > 21:\n",
    "        return sum(agent)\n",
    "    return sum(agent) + 10\n",
    "\n",
    "def get_reward():\n",
    "    global player, dealer\n",
    "    p, d = sum_of_cards(player), sum_of_cards(dealer)\n",
    "    if p > 21 and d > 21: return 0\n",
    "    if p == d: return 0\n",
    "    if p > 21: return -1\n",
    "    if d > 21: return +1\n",
    "    if p > d: return +1\n",
    "    return -1\n",
    "\n",
    "def exit(errno = 0):\n",
    "    sys.exit(errno)\n",
    "    \n",
    "num_of_episodes = 10000\n",
    "num_of_episodes = 1000000\n",
    "x = 0\n",
    "y = 0\n",
    "for num in range(0, num_of_episodes):        \n",
    "    \n",
    "    # random walk\n",
    "    reset_global_variables()\n",
    "    init_deck()\n",
    "    shuffle('player', player, 2)\n",
    "    shuffle('dealer', dealer, 2)\n",
    "    initiate_player_state()\n",
    "    random_generator()\n",
    "    reward = get_reward()\n",
    "    \n",
    "    # update q_results\n",
    "    size = len(episodes)\n",
    "    for episode in episodes:\n",
    "        usable_ace_player, player_sum, dealer_card1, action = episode\n",
    "        state = (usable_ace_player, player_sum, dealer_card1)\n",
    "        state_action = episode\n",
    "        value = reward / size\n",
    "        \n",
    "        if state_action in q_returns:\n",
    "            q_returns.get(state_action).append(value)\n",
    "        else:\n",
    "            q_returns[state_action] = [value]\n",
    "        \n",
    "        # update p_returns\n",
    "        _stick = (usable_ace_player, player_sum, dealer_card1, False)\n",
    "        stick = np.mean(q_returns.get(_stick, [0]))\n",
    "        _hit = (usable_ace_player, player_sum, dealer_card1, True)\n",
    "        hit = np.mean(q_returns.get(_hit, [0]))\n",
    "        p_returns[state] = \"HIT\" if hit > stick else \"STK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "def print_q_returns():\n",
    "    for state, value in sorted(q_returns.items()):\n",
    "        usable_ace_player, player_sum, dealer_card1, action = state\n",
    "        mean = np.mean(value)\n",
    "        print(state, mean, value)\n",
    "print(len(q_returns.keys()))\n",
    "# print(q_returns.keys())\n",
    "# print_q_returns()\n",
    "\n",
    "def print_p_returns():\n",
    "    for state, policy in sorted(p_returns.items()):\n",
    "        usable_ace_player, player_sum, dealer_card1 = state\n",
    "        print(state, policy)\n",
    "print(len(p_returns.keys()))\n",
    "# print(p_returns.keys())\n",
    "# print_q_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dealer_vs_player(usable_ace_flag):\n",
    "    for player_sum in range(21, 11, -1):\n",
    "        for dealer_card1 in range(1, 11):\n",
    "            state = (usable_ace_flag, player_sum, dealer_card1)\n",
    "            action = p_returns.get(state, 'HIT')\n",
    "            if action == None: action = '-'\n",
    "            sys.stdout.write('{}\\t'.format(action))\n",
    "        sys.stdout.write('{}\\n'.format(player_sum))\n",
    "    for dealer_showing in range(1, 11):\n",
    "        sys.stdout.write('{}\\t'.format('HIT'))\n",
    "    sys.stdout.write('{}\\n'.format(11))\n",
    "    for dealer_showing in range(1, 11):\n",
    "        sys.stdout.write('{}\\t'.format(dealer_showing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t21\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t20\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t19\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tHIT\tSTK\t18\n",
      "HIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\t17\n",
      "HIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\t16\n",
      "HIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\t15\n",
      "HIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\t14\n",
      "HIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\t13\n",
      "HIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\t12\n",
      "HIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\t11\n",
      "1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t"
     ]
    }
   ],
   "source": [
    "dealer_vs_player(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t21\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t20\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t19\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t18\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t17\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t16\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t15\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t14\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t13\n",
      "STK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\tSTK\t12\n",
      "HIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\tHIT\t11\n",
      "1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t"
     ]
    }
   ],
   "source": [
    "dealer_vs_player(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=[1,2,3,4,5,6,7,8,9,10]\n",
    "print(np.mean(x))\n",
    "\n",
    "d = {}\n",
    "d[('1_1',True)] = 1.1\n",
    "print(d)\n",
    "\n",
    "key=((1,2),\"string\")\n",
    "value=*(1,2),\"string\"\n",
    "print(key)\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5.2: Soap Bubble\n",
    " 아래 그림과 같이 폐 루프를 형성하는 와이어 프레임이 비눗물로 덩어리져 있고, 와이어 프레임에 그 가장자리에서 비누 표면 또는 기포를 형성한다고 가정 했을 때, \n",
    " > 와이어 프레임의 기하학적 모양이 불규칙하지만 알려진 경우 표면의 모양을 어떻게 계산할 수 있습니까?\n",
    " ![bubble](images/bubble.png)\n",
    "\n",
    " 이 모양에는 인접한 점이 작용하는 각 점의 총 힘이 0이되는 속성이 있습니다 (그렇지 않으면 모양이 변경됩니다). 이것은 어떤 점에서 표면의 높이가 그 점을 중심으로 작은 원의 점에서 높이의 평균임을 의미합니다. 또한 표면은 와이어 프레임과의 경계에서 만나야합니다.\n",
    "\n",
    " 이런 종류의 문제에 대한 일반적인 접근법은 표면으로 덮인 영역 위에 그리드를 놓고 반복 계산에 의해 그리드 지점에서의 높이를 계산하는 것입니다. 그리드 경계에서의 점들은 와이어 프레임으로 가야하고 다른 모든 점은 네 개의 가장 가까운 이웃의 높이의 평균을 향해 조정됩니다.\n",
    "> 이 프로세스는 DP의 반복적 인 정책 평가와 마찬가지로 반복되고 궁극적으로 원하는 표면에 가까운 근사치로 수렴됩니다.\n",
    "\n",
    " 이는 몬테카를로 방법이 원래 설계된 문제의 종류와 유사합니다. 위에서 설명한 반복 계산 대신 표면에 서서 랜덤 보행을 하면서 경계에 도달 할 때까지 동일한 확률로 격자 점에서 인접한 격자 점으로 무작위로 스테핑하는 것을 상상해보십시오.\n",
    "\n",
    "> 경계에서 높이의 예상 값은 시작점에서 원하는 표면의 높이에 가까운 근사치입니다 (실제로는 정확히 위에서 설명한 반복 방법에 의해 계산 된 값입니다). 따라서, 그 지점에서 시작된 많은 걷기의 경계 높이를 단순히 평균화함으로써 한 지점에서의 표면 높이를 근사 적으로 근사시킬 수 있습니다. 한 점의 값 또는 작은 점 집합에 관심이있는 경우이 Monte Carlo 방법은 지역 일관성에 기반한 반복 방법보다 훨씬 효과적 일 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Monte Carlo Estimation of Action Values\n",
    "> 여태까지 순차적 문제는 MDP 라는 방법론을 통해 해결할 수 있고, 이는 agent 의 상호작용을 통해 environment 의 state-transition 정보를 그대로 모방한 dynamics 를 통해 접근하였습니다. 즉 '모델'이란 복잡한 environment 에서 agent 가 state 별로 action 을 취했을 때에 어떤 state 로 변화하는 지에 대한 정보를 말하며 model 의 존재유무는 $p(s',r\\mid s,a)$ 의 알고있는지 여부를 말한다고 할 수 있습니다.\n",
    "\n",
    "  모델이 있다면 DP에서 다루었던 것처럼 미리 가치 함수가 계산 되어 있어서 한 단계 앞의 값을 알 수 있으면 '벨만 방정식'을 통하여 최적의 결과를 예측할 수 있으므로 'state values' 만으로도 policy 를 결정하는 것은 충분합니다. 하지만 모델이 없다면 'state values' 대신 'action values'를 추정하는 편이 유용할 것입니다.\n",
    "\n",
    "> 이 표현을 다시 이해해 보면, 보상은 이미 알고 있으나 환경의 상태 전이를 알 수 없는 경우가 있을텐데요, '블랙잭'의 예제에서와 같이 딜러가 동일한 상태에서 행동할 수 있는 상황이 너무 다양하여 '가치 함수'를 미리 계산하는 것이 어렵기 때문에 'action values'를 이용하여 추정할 수 밖에 없다는 의미로 해석됩니다.\n",
    "\n",
    " 즉, 모델이 없다면 'state values' 만으로는 정책을 결정하기는 어렵기 때문에 주요 목적 가운데 하나는 '몬테 카를로 메소드'를 통해 $q_*$ 함수를 예측하는 것이 목적이며, 우선 'action values'를 얻기 위한 _policy evaluation_ 문제를 해결해야 합니다.\n",
    "\n",
    "  'policy evaluation' 문제는 정책 $\\pi$를 따르는 상태 s 에서 액션 a 를 취했을 때에 예상되는 'return' 값인 $q_\\pi(s,a)$를 추정 하는 것인데 (여태까지 벨만 방정식과 같은 내용은 언급되지 않았습니다.) '몬테 카를로 메소드'는 'state-action'을 다룬다는 점만 제외하면 'state valuse'와 동일하며, state-action 쌍에 대한 모든 경우에 대한 보상의 평균을 계산하는 'every-visit MC method'와 첫 번째 발생하는 경우에 대한 보상만 계산하는 'first-visit MC method' 두 가지가 있습니다. 이 두 메소드들은 무한히 반복을 거듭하는 경우에 'quadratically converge' 합니다.\n",
    "  \n",
    "> 다만 문제점은 많은 'state-action' 쌍을 방문할 수 없다는 점인데요, 만약에 $\\pi$가 'deterministic policy' 임을 가정했을 때에, 각 state 에서 여러 actions 가운데 하나의 action 에 대해서만 returns 를 관찰하게 될 수 있게되고, average 값에 대한 returns 가 없는 경우에는 경험을 통한 개선이 없을 수 있다는 점이 큰 단점으로 존재합니다.\n",
    "  \n",
    " 이것은 _maintaining exploration_ 의 일반적인 문제인데, _k-armed bandit problem_ 에서는 시작 시에 초기값을 높게 주어 랜덤으로 선택될 수 있게 만들어 탐험을 할 수 있는 방법을 썼는데, 모든 state-action 쌍에 대해서 0 이상의 확률을 가지도록 하여 시작 을 random 으로 수행하여 explore 효과를 낼 수 있도록 하고, 무한이 반복하는 경우 유사한 효과를 낼 수 있고 이것을 _exploring starts_ 라고 부릅니다.\n",
    " \n",
    "> 탐험을 통한 시작은 아주 유용하지만 실제 상호작용하면서 학습하는 경우에는 이러한 일반적인 방법으로는 제대로 동작하지 않기 때문에, 각 상태 마다 모든 actions 들이 선택될 수 있도록 _nonezero probability_ 의 stochastic policies 를 고려해볼 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Monte Carlo Control without Exploring Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Off-policy Prediction via Importance Sampling\n",
    "> 모든 learning control 방법들의 딜레마는 최적의 행동을 하기위해 action value 함수를 취하는 반면, 최적의 가치함수를 얻기 위해 많은 non-optimal 한 탐험을 해야하는 것이 그것인데, 어떻게 탐험을 하면서도 최적의 최적의 정책을 학습할 수 있을까요?\n",
    "\n",
    "### Approarch to use two different policies\n",
    "#### 1. target policy : optimal policy 를 학습하기 위한 정책\n",
    "#### 2. behavior policy : 다양한 학습을 위한 behavior 를 생성하기 위한 정책\n",
    "> 여기서 기존의 on-policy 정책은 최종 target policy 위(on) 에서 학습을 수행하지만, off-policy 방식은 target policy 와 떨어진 (off) 별도의 behavior policy 를 통해서 탐험과 학습을 할 수 있다는 의미에서 _off-policy learning_ 이라 부릅니다.\n",
    "\n",
    "### differences between _off-policy learning_ and on-policy\n",
    "#### 1. greater variance and slower\n",
    "#### 2. more powerful and general\n",
    "> 이번 장에서는 _prediction_ 문제만을 다룰 것이므로 (improvement 는 없다, 정책 update 는 없다라는 의미입니다.) target, behavior policy 는 사전에 정의된 고정된 값으로 다룹니다. 또한 $b \\neq \\pi$ 입니다. 그리고 target policy 에서 발생 가능한 것은 behavior policy 에서도 일어나야지 학습이 가능할 것이므로 _coverage_ 에 대한 $\\pi(a\\mid s) > 0\\ implies\\ b(a\\mid s)$를 가정 합니다. 또한 일반적으로 target policy 의 경우 deterministic greedy policy 인 경우가 많으나, behavior policy 의 경우는 $\\epsilon-greedy\\ policy$ 와 같은 stochastic and more exploratory 해야만 coverage 를 유지할 수 있을 것입니다.\n",
    "\n",
    "### _Importance Sampling_\n",
    "> 모든 off-policy 는 importance sampling 방법을 활용하는데, 이는 주어진 다른 샘플들의 임의의 분포를 하의 expected value 들을 예측하는 기법입니다. 그리고 **_importance sampling_** 기법을 targe and behavior policies 들 아래에 발생하는 trajectories 의 상대적인 확률에 의한 반환값의 가중치를 off-policy learning 에 적용하려고 합니다.\n",
    "\n",
    "> target policy 를 통한 expected return 값은 bias 가 많을 것이나, 이 값에 대하여 transition probabilities 의 ratio 값을 곱하여 target policy 의 expected return 값으로 활용할 수 있습니다.\n",
    "$$\n",
    "E[p_{t:T-1}\\ G_t\\mid S_t=s]\\ =\\ v_\\pi(s).\\ (5.4)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Incremental Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Off-policy Monte Carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 *Discounting-aware Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 *Per-decision Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
