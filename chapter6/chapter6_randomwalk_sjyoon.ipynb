{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of 6.2 Random Walk 2018-09-11 08:07:59.719521\n",
      "Start of 6.3 Random Walk Under Batch Updating 2018-09-11 08:08:10.760899\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example 6.2 Random Walk & Example 6.3: Random walk under batch updating\n",
    "Author : SeongJin Yoon\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "class Plot():\n",
    "    def __init__(self, nrows, ncols, figsize):\n",
    "        self.fig_idx = 0\n",
    "        self.len_axes = nrows*ncols\n",
    "        \n",
    "        _, self.axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "        plt.subplots_adjust(wspace=0.1, hspace=0.2)\n",
    "        if self.len_axes != 1:\n",
    "            self.axes = self.axes.flatten()\n",
    "\n",
    "    def get_axes(self, idx):\n",
    "        if self.len_axes != 1: \n",
    "            ax = self.axes[self.fig_idx]\n",
    "        else:\n",
    "            ax = self.axes\n",
    "        return ax\n",
    "    \n",
    "    def draw_mline_begin(self, labels):       \n",
    "        ax = self.get_axes(self.fig_idx)\n",
    "        \n",
    "        ax.set_xlabel(labels[0], fontsize=30)\n",
    "        ax.set_ylabel(labels[1], fontsize=30)\n",
    "        ax.set_title(labels[2], fontsize=30)\n",
    "        \n",
    "        return ax\n",
    "        \n",
    "    def draw_mline(self, ax, pos, data, label, linestyle='solid'):\n",
    "        lines = ax.plot(pos, data, label=label, linestyle=linestyle)\n",
    "        \n",
    "        pivot_idx = random.randint(0, (len(data)*2)//3)\n",
    "        x_pos = pos[pivot_idx]\n",
    "        y_pos = data[pivot_idx]\n",
    "        ax.text(x_pos, y_pos, label, color = lines[0].get_color(), size=18)\n",
    "\n",
    "    def draw_mline_end(self):       \n",
    "        self.fig_idx += 1\n",
    "\n",
    "    def draw_bar(self, pos, data, labels):\n",
    "        ax = self.get_axes(self.fig_idx)\n",
    "        ax.bar(pos, data)\n",
    "        \n",
    "        ax.set_xlabel(labels[0], fontsize=30)\n",
    "        ax.set_ylabel(labels[1], fontsize=30)\n",
    "        ax.set_title(labels[2], fontsize=30)\n",
    "        self.fig_idx += 1\n",
    "        \n",
    "    def show(self):\n",
    "        plt.show()\n",
    "\n",
    "class Action():\n",
    "    left = 0\n",
    "    right = 1\n",
    "    \n",
    "    @staticmethod\n",
    "    def to_str(action):\n",
    "        assert action == Action.left or action == Action.right \n",
    "        strlist = [\"left\", \"right\"]\n",
    "        return strlist[action]\n",
    "\n",
    "class RandomWalk():\n",
    "    def __init__(self):\n",
    "        self.state_list = ['LM', 'A', 'B', 'C', 'D', 'E', 'RM']\n",
    "        self.start = 'C'\n",
    "        self.start_idx = self.state_list.index(self.start)\n",
    "        self.terminal = ['LM', 'RM']\n",
    "        self.init_policy()\n",
    "        self.true_value = [0, 1/6, 2/6, 3/6, 4/6, 5/6, 0]\n",
    "    \n",
    "    def init_policy(self):\n",
    "        self.policy = np.full((7,2), 0.5)\n",
    "        \n",
    "        # terminal state\n",
    "        for state in self.terminal:\n",
    "            state_idx = self.state_list.index(state)\n",
    "            self.policy[state_idx, Action.left] = 0\n",
    "            self.policy[state_idx, Action.right] = 0\n",
    "\n",
    "    def init_value(self):\n",
    "        V = np.full(7, 0.5)\n",
    "        for state in self.terminal:\n",
    "            state_idx = self.state_list.index(state)\n",
    "            V[state_idx] = 0\n",
    "    \n",
    "        return V\n",
    "    \n",
    "    def get_reward(self, state_idx):\n",
    "        state = self.state_list[state_idx]\n",
    "        if state == 'RM':\n",
    "            return 1\n",
    "        return 0\n",
    "    \n",
    "    def is_terminal(self, state_idx):\n",
    "        state = self.state_list[state_idx]\n",
    "        if state in self.terminal:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_action_idx(self, state_idx):\n",
    "        best_idx = np.argmax(self.policy[state_idx])\n",
    "        best = self.policy[state_idx, best_idx]\n",
    "        max_list = np.argwhere(self.policy[state_idx] == best)\n",
    "        max_list = max_list.flatten()\n",
    "        action_idx = max_list[random.randint(0,len(max_list)-1)]\n",
    "        return action_idx\n",
    "        \n",
    "    def do_action(self, state_idx, action_idx):\n",
    "        \n",
    "        if self.is_terminal(state_idx):\n",
    "            return state_idx, self.reward()\n",
    "            \n",
    "        next_state_idx = 0\n",
    "        if action_idx == Action.left:\n",
    "            next_state_idx = state_idx - 1\n",
    "        elif action_idx == Action.right:\n",
    "            next_state_idx = state_idx + 1\n",
    "\n",
    "        return next_state_idx, self.get_reward(next_state_idx)\n",
    "    \n",
    "     \n",
    "    def temporal_difference(self, V, alpha, batch=False):\n",
    "        discount = 1\n",
    "        state_idx = self.start_idx\n",
    "        trajectory = []\n",
    "        \n",
    "        while self.is_terminal(state_idx) is False:\n",
    "            \n",
    "            action_idx = self.get_action_idx(state_idx)\n",
    "            next_state_idx, reward = self.do_action(state_idx, action_idx)\n",
    "\n",
    "            if batch:\n",
    "                trajectory.append([state_idx, reward])\n",
    "            else:\n",
    "                error = reward + discount*V[next_state_idx] - V[state_idx]\n",
    "                V[state_idx] += alpha*error\n",
    "\n",
    "            state_idx = next_state_idx\n",
    "        \n",
    "        trajectory.append([state_idx, 0])\n",
    "        \n",
    "        return trajectory\n",
    "\n",
    "    \n",
    "    def batch_updating(self, method, V, trajectories, alpha):        \n",
    "        discount = 1\n",
    "        count = 1\n",
    "        while True:\n",
    "            acc_error = np.zeros(7)\n",
    "            for trajectory in trajectories:\n",
    "                \n",
    "                if method == 'TD':\n",
    "                    for step in range(len(trajectory)-1):\n",
    "                        state_idx, reward = trajectory[step]\n",
    "                        next_state_idx, _ = trajectory[step+1]\n",
    "                        error = reward + discount*V[next_state_idx] - V[state_idx]\n",
    "                        acc_error[state_idx] += error\n",
    "                else:\n",
    "                    for step in range(len(trajectory)-1):\n",
    "                        state_idx, discounted_return = trajectory[step]\n",
    "                        error = discounted_return - V[state_idx] \n",
    "                        acc_error[state_idx] += error\n",
    "                        \n",
    "            if np.sum(np.abs(acc_error*alpha)) < 1e-3:\n",
    "                break\n",
    "\n",
    "            V += acc_error*alpha\n",
    "            assert V[0] == 0 and V[6] == 0\n",
    "\n",
    "            count += 1\n",
    "\n",
    "    def constant_alaph_mc(self, V, alpha, batch=False):\n",
    "        \n",
    "        episode, reward = self.generate_episode()\n",
    "        \n",
    "        discount = 1\n",
    "        discounted_return = 0\n",
    "        next_reward = reward\n",
    "        trajectory = []\n",
    " \n",
    "        for step in range(len(episode)-1, -1, -1):\n",
    "            state_idx, action_idx, reward = episode[step]\n",
    "            discounted_return = next_reward + discount*discounted_return\n",
    "            if batch:\n",
    "                trajectory.append([state_idx, discounted_return])\n",
    "            else:\n",
    "                error = discounted_return - V[state_idx]\n",
    "                V[state_idx] += alpha*error\n",
    "                \n",
    "            next_reward = reward\n",
    "            \n",
    "        return trajectory            \n",
    "            \n",
    "    def generate_episode(self):           \n",
    "        episode = []\n",
    "        state_idx = self.start_idx\n",
    "        reward = 0\n",
    "        while True:\n",
    "            action_idx = self.get_action_idx(state_idx)\n",
    "            episode.append([state_idx, action_idx, reward])\n",
    "            next_state_idx, reward = self.do_action(state_idx, action_idx)\n",
    "            if self.is_terminal(next_state_idx):\n",
    "                break\n",
    "            state_idx = next_state_idx\n",
    "\n",
    "        return episode, reward\n",
    "    \n",
    "    def calc_rms_error(self, V):\n",
    "        \n",
    "        error = (self.true_value - V)[1:len(V)]\n",
    "        rms_error =  math.sqrt(np.mean(error**2))\n",
    "            \n",
    "        return rms_error\n",
    "    \n",
    "    def draw_rms_plot(self, ax, method, alpha, alpha_fmt=\"%.2f\", batch=False):\n",
    "        num_runs = 100\n",
    "        num_episode = 100+1\n",
    "        episode_list = list(range(num_episode))\n",
    "        \n",
    "        avg_rms_error = np.zeros(num_episode)\n",
    "        \n",
    "        for run in range(0, num_runs):\n",
    "            V = self.init_value()\n",
    "            rms_error_list = np.zeros(num_episode)\n",
    "            trajectories = []\n",
    "            \n",
    "            for episode_idx in range(num_episode):\n",
    "                if method == 'TD':\n",
    "                    trajectory = self.temporal_difference(V, alpha, batch)\n",
    "                else:\n",
    "                    trajectory = self.constant_alaph_mc(V, alpha, batch)\n",
    "                \n",
    "                if batch:\n",
    "                    trajectories.append(trajectory)\n",
    "                    self.batch_updating(method, V, trajectories, alpha)\n",
    "                    \n",
    "                rms_error_list[episode_idx] = self.calc_rms_error(V)\n",
    "                \n",
    "            avg_rms_error += rms_error_list\n",
    "   \n",
    "        avg_rms_error /= num_runs\n",
    "        \n",
    "        label = method + \" a = \" + alpha_fmt % (alpha)\n",
    "        linestyle = 'dashdot' if method == 'TD' else 'solid'\n",
    "        self.plot.draw_mline(ax, episode_list, avg_rms_error, label, linestyle)\n",
    "        \n",
    "    def draw_td_value_plot(self):\n",
    "        labels = ['State', 'Estimated Value', \n",
    "                    'Values learned after various numbers of episodes']\n",
    "        ax = self.plot.draw_mline_begin(labels)   \n",
    "    \n",
    "        alpha = 0.1\n",
    "        \n",
    "        num_episode = 100+1\n",
    "        V = self.init_value()\n",
    "    \n",
    "        for episode_idx in range(num_episode):\n",
    "            self.temporal_difference(V, alpha)\n",
    "            label = \"# of episode = %d\" % (episode_idx)\n",
    "            if episode_idx in [0, 1, 10, 100]:\n",
    "                self.plot.draw_mline(ax, self.state_list[1:-1], V[1:-1], label)\n",
    "        \n",
    "        label = \"True Values\"\n",
    "        self.plot.draw_mline(ax, self.state_list[1:-1], \n",
    "                        self.true_value[1:-1], label)\n",
    "        self.plot.draw_mline_end()\n",
    "        \n",
    "    def draw_online_plot(self):\n",
    "        self.plot = Plot(nrows=1, ncols=2, figsize=(30,10))\n",
    "       \n",
    "        # left figure\n",
    "        self.draw_td_value_plot()\n",
    "    \n",
    "        # right figure\n",
    "        labels = ['Walks/Episodes', 'RMS error', \n",
    "                    'Empirical RMS error averaged over states']\n",
    "        ax = self.plot.draw_mline_begin(labels)   \n",
    "        \n",
    "        for alpha in [0.15, 0.1, .05]:\n",
    "            self.draw_rms_plot(ax, 'TD', alpha)\n",
    "\n",
    "        for alpha in [0.01, 0.02, 0.03, 0.04]:\n",
    "            self.draw_rms_plot(ax, 'MC', alpha)\n",
    "                \n",
    "        self.plot.draw_mline_end()\n",
    "\n",
    "\n",
    "    def draw_batch_plot(self):\n",
    "        \n",
    "        self.plot = Plot(nrows=1, ncols=2, figsize=(30,10))\n",
    "        \n",
    "        labels = ['Walks/Episodes', 'RMS error', 'Batch Training']\n",
    "        ax = self.plot.draw_mline_begin(labels)   \n",
    "\n",
    "        # TD batching update\n",
    "        alpha = 0.001\n",
    "        self.draw_rms_plot(ax, 'TD', alpha, \"%.3f\", True)\n",
    "\n",
    "        # MC batching update\n",
    "        self.draw_rms_plot(ax, 'MC', alpha, \"%.3f\", True)\n",
    "\n",
    "        self.plot.draw_mline_end()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    begin_time = dt.datetime.now()\n",
    "    print(\"Start of 6.2 Random Walk\", begin_time)\n",
    "    randomwalk = RandomWalk()\n",
    "    randomwalk.draw_online_plot()\n",
    "    \n",
    "    print(\"Start of 6.3 Random Walk Under Batch Updating\", dt.datetime.now())\n",
    "    s_time = dt.datetime.now()\n",
    "    randomwalk.draw_batch_plot()\n",
    "    \n",
    "    end_time = dt.datetime.now()\n",
    "    print()\n",
    "    print(\"End of Random Walk\", end_time)\n",
    "    print(\"Running Time\", end_time - begin_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLGosu",
   "language": "python",
   "name": "rlgosu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
