{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9. On-policy Prediction with Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.11 Looking Deeper at On-policy Learning: Interest and Emphasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이번 장에서 다룬 알고리즘들은 모든 상태를 동일한 중요도로 다루지만, 사실 일부 특수한 상태에만 관심을 가집니다.\n",
    "* Discounted episodic problem 경우 최신 상태에 더 가치를 두고 reward 감쇄를 통해 중요도가 반영된다고 할 수 있습니다.\n",
    "* Action value function 경우에도, greedy action 에 비해 poor action 이 덜 중요합니다\n",
    "* Function approximation 은 항상 리소스가 제약되는데 목표에 좀 더 집중할 수 있다면 성능은 더욱 개선될 수 있을 것입니다.\n",
    "> 1. 관심을 두는 특정 state or state-action 에 대해서만 가중치를 줄 수 있다면 더 좋은 성능이 예상됩니다\n",
    "<br>\n",
    "\n",
    "** 결국 이 장에서는 function approximation 을 다루는데 observations 를 어떻게 바라볼 것인지 입니다. **\n",
    "> 1. 첫 번째는 여태까지 해 왔던 모든 상태는 동일한 가중치로 바라보는 관점이고\n",
    "> 2. 두 번째는 on-policy distribution 즉 현재 정책을 따를 때의 observed distribution 을 Interest 와 Emphasis 관점으로 접근하는 관점입니다.\n",
    "<br>\n",
    "\n",
    "### 모든 상태를 동일한 가중치로 바라보는 관점\n",
    "* 모든 상태를 동일하게 다루는 한 가지 이유는 On-policy distribution 을 따라 업데이트 하기 때문입니다.\n",
    "* (?) Semi-gradient methods 를 통해 더 나은 이론적 결과를 가능하게 하는  ... \n",
    "* On-policy distribution 은 target policy를 따르는 하나의 MDP 환경에서 만날 수 있는 분포를 말합니다.\n",
    "* 이러한 상황을 일반화하여, MDP를 위한 on-policy 정책을 하나가 아니라 많이 가질 수도 있습니다 \n",
    "* Target policy를 따르는 trajactories 들의 상태 분포라는 점에서 모든 policy 들은 공통점을 가진다.\n",
    "* 다만, 어떻게 초기화 되었는 지에 따라 다양한 결과를 가질 수 있다\n",
    "> 가중치를 일관되게 적용하는 On-policy 분포에 따라 샘플링의 방법들이 여태까지의 접근이었다\n",
    "<br>\n",
    "\n",
    "### Interest 와 Emphasis 기준으로 바라보는 관점\n",
    "#### 1. Non-negative scalar measure random variables, Interest\n",
    "> time t 에서 value function 을 정확히 어떤 특정 state 혹은 state-action 에 대해서만 관심을 두는 정도, 관심이 전혀 없다면 0, 완전히 관심을 가진다면 1.\n",
    "> 이러한 interest 값은 다양한 인과관계로 표현할 수 있는데, time t 에 이르는 trajactory 혹은 time t 에서 학습된 parameters 가 될 수도 있습니다.\n",
    "> VE(Value Error)에서 분포 $\\mu$ 는 target policy에 의한 상태들의 분포에 의해 정의되어지고, interest에 의해 가중치를 부여받는다.\n",
    "\n",
    "<br>\n",
    "* The on-policy distribution in episodic tasks - 관심사에 더 많은 시간을 투자한다\n",
    "![on_policy_distribution_in_episodic_tasks](images/on_policy_episodic.png)\n",
    "\n",
    "#### 2. Anothor non-negative scalar random variables, Emphasis $M_t$\n",
    "> learning update 를 배가하고, time t 에서 완료된 학습을 강조하거나 혹은 덜 강조합니다.\n",
    "\n",
    "<br>\n",
    "#### general n-step learning rule 은 다음과 같습니다.\n",
    "$$\n",
    "w_{t+n}\\doteq w_{t+n-1} + \\alpha M_t [ G_{t:t+n} - \\hat{v}(S_t, w_{t+n-1})] \\nabla \\hat{v}(S_t, w_{t+n-1}),\\ \\ \\  0\\leq t < T, (9.25)\n",
    "$$\n",
    "$$\n",
    "M_t = I_t + \\gamma^n M_{t-n},\\ \\ \\ 0 \\leq t < T, (9.26)\n",
    "$$\n",
    "<br>\n",
    "> 위의 식에서와 같이 관심을 가지는 state 에 대해서만 확률변수에 의해 더욱 많은 보상을 받게되고 weight decay 영향을 포함하고, 모든 업데이트는 에피소드의 마지막에 바영된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.4: Interest and Emphasis\n",
    "#### Interest 와 Emphsis 를 이용한 four-state Markov reward process\n",
    "\n",
    "##### 1. 에피소드는 가장 왼쪽 state 에서 시작합니다\n",
    "##### 2. 우측으로 한 칸 이동 시에 +1 보상을 받고 가장 우측이 terminal state 입니다\n",
    " ##### 3. True value 값들은 좌측에서부터 4, 3, 2, 1 이며, parameter 통한 근사 추정을 합니다\n",
    " ##### 4. 첫 번째, 두 번째 state 들과 세번째 네번째 state 들은 각 각 $w_1, w_2$ 으로 값을 추정합니다\n",
    " ##### 5. True value 값과 parameters 와는 다를 수 있으나, 관심 state 는 leftmost state 하나입니다\n",
    " \n",
    " #### gradient Monte Carlo algorithms\n",
    " ##### 1. Interest & Emphasis 미적용 시에는 $w_{\\infty} = (3.5, 1.5)$ \n",
    " ##### 2. Interest & Emphasis 적용 시에는 첫 번째 state 값은 True Value 인 4에 수렴합니다\n",
    " \n",
    " #### two-step semi-gradient TD methods\n",
    " ##### 1. Interest & Emphasis 미적용 시에는 $w_{\\infty} = (3.5, 1.5)$ \n",
    " ##### 2. Interest & Emphasis 적용 시에는  $w_{\\infty} = (4, 2)$  값을 얻었습니다\n",
    " \n",
    "> 결국에는 naive 한 학습 보다는 특정 state 에 좀 더 가중치를 두고 학습하는 경우에 적어도 해당 state 에 대해서는 더 정확한 결과를 얻을 수 있다는 의미인 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q&A\n",
    "### 1. On-policy vs. Off-policy ?\n",
    "강화학습의 목표는 MDP 를 통해 \"순차적 행동 결정문제\"를 해결하는 것.\n",
    "다시 말해 어떠한 상태에서 최적의 정책 즉 행동을 정하는 문제라고 볼 수 있습니다\n",
    "<br>\n",
    "#### 1-1. On-policy : SARSA\n",
    "* 강화학습은 GPI (Generalized Policy Iteration) 즉, 정책 평가(Policy Evaluation)과 정책 발전(Policy Development) \n",
    "* 벨만 기대 방정식을 이용하여 현재의 정책에 대한 참 가치함수를 구하는 것이 \"정책 평가\", 업데이트가 \"정책 발전\"\n",
    "* 이에 \"정책 평가\" 과정에서 Monte Carlo Estimation or Time Difference Estimation 기법을 사용합니다.\n",
    "* 하지만 환경의 모델인 P를 모르기 때문에 가치함수 대신 Q함수를 이용합니다. \n",
    "* On-policy Temporal-Difference Control 이며 다른 말로 SARSA 라고도 합니다.\n",
    "* ** 살사는 탐욕 정책에 따른 행동을 결정하고, 이에 따른 정책도 바로 반영되는 것이 가장 큰 단점 **\n",
    "* ** 결과적으로 SARSA는 에이전트와 환경의 상호작용 과정에서 Q함수 자체(on-policy)를 업데이트 합니다 **\n",
    "> 1. 큐함수를 사용한 탐욕정책은 $\\pi(s) = argmax_{a\\in A}Q(s,a)$ 이고 \n",
    "> 2. 시간차 제어의 식은 $Q(S_t,A_t) = Q(S_t,A_t) + \\alpha(R + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t,A_t))$ 이며,\n",
    "> 3. Q함수를 업데이트 하기 위해서 $[ S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1} ]$를 샘플로 사용합니다.\n",
    "> 4. 이미 학습된 에이전트라면 탐욕 정책이 좋을 수 있으나 초반의 탐험이 필수이므로 $\\epsilon$ 을 사용합니다\n",
    "<br>\n",
    "\n",
    "#### 1.-2. Off-policy : Q-learning\n",
    "* 살사의 경우 바로 다음의 state 에 대한 보상값을 모델에 바로 반영을 하기 때문에 갇히는 경우가 발생합니다\n",
    "* 즉, 살사는 정책이 따로 존재하지 않으며, 큐함수에 따라 행동하는 것이 정책이었습니다.\n",
    "* 탐험할 수 있는 충분한 시간을 보장 받지 못 하고 모델을 업데이트하는 것을 피하기 위한 Off-policy TD\n",
    "* 행동하는 정책과 학습하는 정책을 분리(Off-policy)하여 즉시 반영되어 갇히는 현상을 피하기 위함입니다\n",
    "* Off-policy Temporal-Difference Control 이며 다른 말로 Q-Learning 이라고 합니다.\n",
    "* ** 다음 큐함수를 최대화 하도록 큐함수를 업데이트 하여 살사의 단점을 보완하였습니다 **\n",
    "* ** 결과적으로 Q-learning 은 행동하는 정책과 큐함수 업데이트하는 정책이 서로 다름(off-policy)니다 **\n",
    "> 1. 큐함수 업데이트는 $Q(S_t,A_t) = Q(S_t,A_t) + \\alpha(R_{t+1} + \\gamma max_{a'} Q(S_{t+1}, a') - Q(S_t,A_t))$ 이다\n",
    "> 2. 큐함수를 벨만 최적 방정식은 $q_*(s,a) = E[R_{t+1} + \\gamma max_a q_* (S_{t+1}, a')\\ |\\ S_t=s, A_t=a] $\n",
    "> 3. 살사와의 가장 큰 차이점은 다음 상태 행동을 하는 것 대신 최대 큐함수의 값으로 업데이트 하게 됩니다\n",
    "> 4. 살사에서는 큐함수 업데이트를 위해 벨만 기대방정식을 사용, 큐러닝은 *벨만 최적 방정식*을 사용합니다\n",
    "<br>\n",
    "\n",
    "#### 2. On-policy distribution ?\n",
    "#### 3. Semi-gradient methods?\n",
    "#### 4. Monte Carlo algorithms\n",
    "#### 5. Two-step semi-gradient TD methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
