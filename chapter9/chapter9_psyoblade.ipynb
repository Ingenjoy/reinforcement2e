{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9. On-policy Prediction with Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.11 Looking Deeper at On-policy Learning: Interest and Emphasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 이번 장에서 다룬 알고리즘들은 모든 상태를 동일한 중요도로 다루지만, 사실 일부 특수한 상태에만 관심을 가집니다.\n",
    "#### 2. Discounted episodic problem 경우 최신 상태에 더 가치를 두고 reward 감쇄를 통해 중요도가 반영된다고 할 수 있습니다.\n",
    "#### 3. Action value function 경우에도, greedy action 에 비해 poor action 이 덜 중요합니다\n",
    "#### 4. Function approximation 은 항상 리소스가 제약되는데 목표에 좀 더 집중할 수 있다면 성능은 더욱 개선될 수 있을 것입니다.\n",
    "#### 5. 모든 상태를 동일하게 다루는 한 가지 이유는 Semi-gradient methods 를 통해 더 나은 이론적 결과를 가능하게 하는 On-policy distribution 을 따라 업데이트 하기 때문입니다.\n",
    "#### 6. On-policy distribution 은 target policy를 따르는 하나의 MDP 환경에서 만날 수 있는 분포를 말합니다.\n",
    "<br>\n",
    "#### 이러한 것들을 일반화 해 보면\n",
    "##### 1. MDP를 위한 on-policy 정책을 하나가 아니라 많이 가지게 한다\n",
    "##### 2. Target policy를 따르는 trajactories 들의 상태 분포라는 점에서 모든 policy 들은 공통점을 가진다. 다만, 어떻게 초기화 되었는 지에 따라 다양한 결과를 가질 수 있다\n",
    "<br>\n",
    "\n",
    "#### 비 음수 스칼라 척도 Interest & Emphasis\n",
    "##### 1. Non-negative scalar measure random variables, Interest\n",
    "> time t 에서 value function 을 정확히 어떤 특정 state 혹은 state-action 에 대해서만 관심을 두는 정도, 관심이 전혀 없다면 0, 완전히 관심을 가진다면 1.\n",
    "> 이러한 interest 값은 다양한 인과관계로 표현할 수 있는데, time t 에 이르는 trajactory 혹은 time t 에서 학습된 parameters 가 될 수도 있는데, VE(Value Error)에서 분포 $\\mu$ 는 target policy에 의한 상태들의 분포에 의해 정의되어지고, interest에 의해 가중치를 부여받는다.\n",
    "\n",
    "<br>\n",
    "#### The on-policy distribution in episodic tasks - 관심사에 더 많은 시간을 투자한다?\n",
    "![on_policy_distribution_in_episodic_tasks](images/on_policy_episodic.png)\n",
    "\n",
    "##### 2. Anothor non-negative scalar random variables, Emphasis $M_t$\n",
    "> learning update 를 배가하고, time t 에서 완료된 학습을 강조하거나 혹은 덜 강조합니다.\n",
    "\n",
    "<br>\n",
    "#### general n-step learning rule 은 다음과 같습니다.\n",
    "$$\n",
    "w_{t+n}\\doteq w_{t+n-1} + \\alpha M_t [ G_{t:t+n} - \\hat{v}(S_t, w_{t+n-1})] \\nabla \\hat{v}(S_t, w_{t+n-1}),\\ \\ \\  0\\leq t < T, (9.25)\n",
    "$$\n",
    "$$\n",
    "M_t = I_t + \\gamma^n M_{t-n},\\ \\ \\ 0 \\leq t < T, (9.26)\n",
    "$$\n",
    "<br>\n",
    "> 위의 식에서와 같이 관심을 가지는 state 에 대해서만 확률변수에 의해 더욱 많은 보상을 받게되고 weight decay 영향을 포함하고, 모든 업데이트는 에피소드의 마지막에 바영된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 9.4: Interest and Emphasis\n",
    "#### Interest 와 Emphsis 를 이용한 four-state Markov reward process\n",
    "\n",
    "##### 1. 에피소드는 가장 왼쪽 state 에서 시작합니다\n",
    "##### 2. 우측으로 한 칸 이동 시에 +1 보상을 받고 가장 우측이 terminal state 입니다\n",
    " ##### 3. True value 값들은 좌측에서부터 4, 3, 2, 1 이며, parameter 통한 근사 추정을 합니다\n",
    " ##### 4. 첫 번째, 두 번째 state 들과 세번째 네번째 state 들은 각 각 $w_1, w_2$ 으로 값을 추정합니다\n",
    " ##### 5. True value 값과 parameters 와는 다를 수 있으나, 관심 state 는 leftmost state 하나입니다\n",
    " \n",
    " #### gradient Monte Carlo algorithms\n",
    " ##### 1. Interest & Emphasis 미적용 시에는 $w_{\\infty} = (3.5, 1.5)$ \n",
    " ##### 2. Interest & Emphasis 적용 시에는 첫 번째 state 값은 True Value 인 4에 수렴합니다\n",
    " \n",
    " #### two-step semi-gradient TD methods\n",
    " ##### 1. Interest & Emphasis 미적용 시에는 $w_{\\infty} = (3.5, 1.5)$ \n",
    " ##### 2. Interest & Emphasis 적용 시에는  $w_{\\infty} = (4, 2)$  값을 얻었습니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A\n",
    "#### 1. On-policy ?\n",
    "#### 2. On-policy distribution ?\n",
    "#### 3. Semi-gradient methods?\n",
    "#### 4. Monte Carlo algorithms\n",
    "#### 5. Two-step semi-gradient TD methods?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [py36]",
   "language": "python",
   "name": "Python [py36]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
