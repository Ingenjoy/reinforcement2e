{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 이 챕터에서는 finite markov decision processes 에 대해 소개한다. bandit problem과 마찬가지로 evaluative feedback 뿐만 아니라 associative aspect 또한 다루고 있다. (다른 situations에서 다른 actions들을 선택) \n",
    " \n",
    " **MDP(Markov Decision Process)**는 전형적인 순차적 의사 결정(sequential decision making)을 말하는데 행위(actions)들이 즉각적인 보상뿐만 아니라, 이후의 상황, 상태 그리고 미래의 보상(subsequent situations , or states, and through thoes future rewards)들에 영향을 준다. 그러므로 즉각적인 보상과 지연된 보상과의 균형(trade-off)을 다룬다.\n",
    " \n",
    " ### Approarch\n",
    "  bandit problem에서는 reward에 따른 action value function을 update하여 $q_*(a)$를 추정하는 문제였으나, MDP에서는 state s에 대한, action a를 선택하는 value function $q_*(s, a)$를 구하는 문제 혹은, 주어진 state에 대한 value function $v_*(a)$ 즉, 주어진 optimal action selection을 추정하는 문제로 바뀌었다. 이와 같이 상태 의존적인 정량적인 접근(state-dependent quantities)은 indivisual action selections 들에 대한 장기적인 결과(long-term consequences)에 좋은 영향을 준다. \n",
    "  \n",
    " 반환값(returns), 가치 함수(value functions), 벨만 방정식(Bellman equations) 과 같은 문제의 수학적인 구조의 핵심 요소에 대해서 설명한다.\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 The Agent-Environment Interface\n",
    "<br>\n",
    " #### *MDPs* ?\n",
    " 학습과 의사결정을 하는 *agent* 그리고 agent 이외의 모든 것을 의미하는 *environment* 이 두 객체의 interaction으로부터 goal 을 달성하기 위한 학습하는 문제를 다룬다.\n",
    " ![interaction.png](images/interaction.png)\n",
    "<center>**Figure 3.1: The agent-environment interaction in a Markov decision process.**</center>\n",
    " \n",
    "a *finite* MDP는 states, actions, and rewards (S, A, and R)들의 sets들이 finite number of elements 로 구성됨을 말하며, random variables $R_t$ and $S_t$ 는 바로 직전의 state, action 에만 종속적인 discrete probability ditributions 을 가진다.\n",
    "<br>\n",
    "#### *Markov property*?\n",
    " 확률변수 $S_t$ and $R_t$ 개별 값들의 확률이 바로 다음의 상태$S_{t-1}$와 액션$A_{t-1}$에만 영향을 주는 경우 Markov 속성을 가졌다고 말한다.\n",
    " $$ P[S_{t+1}\\ |\\ S_t] = P[S_{t+1}\\ |\\ S_1, ... S_t]$$\n",
    "##### The state captures all relevant information from the history\n",
    "##### Once the state is known, the history may be thrown away\n",
    "##### i.e. The state is a su\u000ecient statistic of the future\n",
    "<br> \n",
    "#### 1. four arguments *dynamics* function\n",
    "$$ p(s',r \\ | \\ s,a) \\doteq Pr \\{S_t=s',R_t=r\\ |\\ S_{t-1}=s,\\ A_{t-1}=a\\} , \\ (3.2)$$\n",
    "<center>MDP의 상태전이의 다양성을 나타낸 수식</center>\n",
    "$$ \\Sigma\\Sigma p(s',r \\ | \\ s,a)  = 1,\\ for\\ all\\ s \\in S, a \\in A(s). \\ (3.3) $$\n",
    "<center> 여기서 p는 모든 s, a 의 경우의 수에 따른 확률분포를 말한다.</center>\n",
    "<br>\n",
    "#### 2. *state-transition probabilities*\n",
    "$$ p(s'\\ |\\ s,a) \\doteq Pr \\{S_t=s'\\ |\\ S_{t-1}=s, A_{t-1}=a \\} = \n",
    "\\begin{equation}\n",
    "\\sum_{r\\in R}p(s',r\\ |\\ s,a)\n",
    "\\end{equation}\\ (3.4)\n",
    "$$\n",
    "<br>\n",
    "#### 3. two arguments *expected rewards for state-action pairs*\n",
    "$$ r(s,a) \\doteq E [R_t\\ |\\ S_{t-1}=s, A_{t-1}=a ] = \n",
    "\\begin{equation}\n",
    "\\sum_{r\\in R}\n",
    "\\sum_{s'\\in S}p(s',r\\ |\\ s,a)\n",
    "\\end{equation},\\  (3.5)\n",
    "$$\n",
    "<br>\n",
    "#### 4. three arguments *expected rewards for state-action-next-state triples*\n",
    "$$ r(s,a,s')\\doteq E [R_t\\ |\\ S_{t-1}=s,\\ A_{t-1}=a,\\ S_t\\ =\\ s'] = \n",
    "\\begin{equation}\n",
    "\\sum_{r\\in R}r \\frac{p(s',r\\ |\\ s,a)}{p(s'\\ |\\ s,a)}\n",
    "\\end{equation},\\  (3.6)\n",
    "$$\n",
    "<br>\n",
    "agent 와 environmetn 경계는 생각보다 agent 쪽에 가깝다. 로봇의 모터와 기계적인 연결 그리고 센싱 하드웨어 등은 모두 환경의 부품이라고 보여지면, 사람이나 동물의 경우에도 근육, 뼈대 및 인식기관 등도 환경의 부분이라고 간주된다. 보상들 또한 에이전트의 외부에서 전달된다, agent 가 environment 에 대해 항상 모르는 것이 아니라 Rubik's cube 맞추기와 같이 상황에 따라 전체 정보를 다 알고 있지만 해결하기 어려운 문제도 있다.\n",
    "\n",
    "##### * 결국, agent 와 environment 의 경계는 agent's 의 absolute control 의 한계이지 에이전트가 알고 있는 정보(knowledge)가 아니다.\n",
    "##### * states 와 actions 는 task 별로 아주 다양한데 어떻게 표현되는 지에 따라서 성능의 차이가 아주 크다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.3 Recycling Robot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Goals and Rewards\n",
    "강화학습의 가장 큰 목표는 reward라고 불리는 scalar signal 값의 expected 누적 합의 최대화를 하는 방법을 배우는 것이다.\n",
    "##### * 목표의 아이디어를 형식화 하기 위한 보상 시그널을 사용하는 것이 강화학습의 가장 큰 특징 중의 하나이다.\n",
    "\n",
    "이러한 이해를 돕는 가장 좋은 방법은 실례를 들어보고 상상하는 것인데 깡통 줍는 로봇이 매 time step 마다 -1 보상을 받게 되고 깡통을 주었을 때에 +1 보상을 배터리가 방전되어 관리자가 옮기게 되는 경우 -3점 등으로 생각할 수 있다. 하지만 무엇 보다도 얻고자 하는 가장 중요한 목적을 전달하는 것이 중요하다.\n",
    "\n",
    "##### * 보상 신호(reward signal)은 agent 의 prior knowledge 를 담는 곳이 아니라 우리가 얻고자 하는 최종 목표를 알려주어야 하는데 체스의 예를 들면 승리하는 것이 목표가 되어야지 상대방의 좋은 말을 획득하는 것이 되어서는 안 된다.\n",
    "##### * The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Returns and Episodes\n",
    "여태까지는 비형식적인 강화학습의 목표를 이야기 했다면 좀 더 형식적으로 표현하면 예상되는 반환값(expected return)이라 볼 수있다.\n",
    "$$ G_t\\ \\doteq R_{t+1}+R_{t+2}+R_{t+3}+\\ ... + R_T,\\ (3.7)$$\n",
    "<br>\n",
    "#### Differences between Goal & Task ?\n",
    "Goal 은 강화학습의 최종 목표이며, 체스에서 승리하거나 미로를 탈출하는 것이 그것이다\n",
    "Task 는 Goal \n",
    "#### Episodic Tasks\n",
    "*terminal state* 라고 불리우는 special state 에 도달하면 episode 가 종료되고, 시작 시점의 상태로 reset 된다.\n",
    "episode가 매번 다르게 종료된다고 하더라도, jk이전의 episode와 다음의 episode는 독립적이다\n",
    "동일한 종료 상태 가지더라도 다른 보상을 받을 수 있다\n",
    "때때로 종료 상태와 비 종료 상태의 구분 또는 종료 시간 T가 확률변수로 사용될 수 있다\n",
    "보상의 경우에 시간이 지남에 따른 감가(*discounting* )이 없다.\n",
    "#### Continuing Tasks\n",
    "정해진 종료 조건이 없고, 무한이 에피소드가 진행되는 경우\n",
    "final time step 이 $T=\\infty$, 이므로 최대화 할 수 없기 때문에 *discounting.* 기법을 적용하여 *discounted return*\n",
    "$$ G_t\\ \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\ ... =\n",
    "\\begin{equation}\n",
    "\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "\\end{equation},\\  (3.8)\n",
    "$$\n",
    "where $0 \\leq \\gamma \\leq 1$, called the *discount rate.*\n",
    "이는 미래의 보상을 현재에 계산하는 방식으로, 늦게 받으면 받을 수록 보상이 작아지는 효과를 가진다\n",
    "$\\gamma < 1$의 값을 가지면 (3.8) 식의 infinite sum 값은 $\\{R_k\\}$ 값에 바운드 되어 finite value를 가진다\n",
    "만약 $\\gamma = 0$이라면 \"myopic\" 즉 근시안 적인 눈 앞의 보상만 바라보는 결과를 가지게 되고,\n",
    "$\\gamma$ 값이 1에 가까워 질 수록 미래의 보상은 점점 커질 것이다.\n",
    "<br>\n",
    "#### reward is a constant +1, then return is\n",
    "$$ G_t = \n",
    "\\begin{equation}\n",
    "\\sum_{k=0}^\\infty \\gamma^k = \\frac{1}{1-\\gamma}.\n",
    "\\end{equation}\\ (3.9)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.4: Pole-Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Unified Notation for Episodic and Continuing Tasks\n",
    "Episodic Task 와 Continuing Task 를 하나의 수식으로 표현하고자 긴 task를 여러개의 episodic task로 생각할 수 있으며 $S_{t,i}$와 같이 i 번째 episode의 time t 를 의미하도록 표기할 수 있으나 때때로 생략할 수도 있다. 아래와 같이 episodic termination 구간을 두고 일정 구간 이후에는 *absorbing state * 상태로 빠지게 되고 보상이 0이 되는 것으로 표현이 가능하다.\n",
    "![absorbing_state](images/absorbingstate.png)\n",
    "#### absorbing state return is\n",
    "$$ G_t \\doteq\n",
    "\\begin{equation}\n",
    "\\sum_{k=t+1}^T \\gamma^{k-t-1}R_k,\n",
    "\\end{equation}\\ (3.11)\n",
    "$$\n",
    "위 식은 아래와 같이 풀어 쓸 수 있다\n",
    "$$G_t\\ \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\ ... + \\gamma^{T-t-1}R_T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Policies and Value Functions\n",
    " 강화학습의 거의 모든 알고리즘에는 *value functions*을 추저아는 과정이 포함되어 있는데 이는 agent 가 *얼마나 좋은 상태로* 이끄는 가를 말하는 것인데 이는 예상할 수 있는 미래의 가치들을 의미하는데 이 가치함수를 만들어내는 데에는 소위 정책(policies)이라고 하는 행동 하는 방법에 대한 가치함수를 말한다.\n",
    "<br>\n",
    "#### **정책** or Policy?\n",
    "##### 상태로부터 선택 가능한 행동들에 이르는 확률\n",
    "##### $\\pi(a\\mid s)$ : $A_t=a\\ if\\ S_t=s$. 즉 상태 s 일때 행동 a를 수행할 확률.\n",
    "##### 강화학습의 방법들은 경험의 결과에 의하여 어떻게 에이전트의 정책이 변경 되는 지에 대한 것이라 말할 수 있다.\n",
    "<br>\n",
    "#### state-value function for policy $\\pi$\n",
    "$$ v_\\pi(s) \\doteq E_\\pi[G_t\\ |\\ S_t=s] = E_\\pi\n",
    "\\begin{bmatrix}\n",
    "\\begin{equation}\n",
    "\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}\\mid S_t=s\n",
    "\\end{equation}\n",
    "\\end{bmatrix}\n",
    ",\\ for \\ all \\ s \\in S,\\ (3.12)\n",
    "$$\n",
    "<br>\n",
    "$v_\\pi(s)$ 는 *state-value function for policy $\\pi$.\n",
    "<br>\n",
    "**상태-가치 함수*** 는 정책함수 $\\pi$를 따르는 agent 의 보상 확률변수 R에 대한 expected value \n",
    "<br>\n",
    "#### action-value function for policy $\\pi$.\n",
    "$$ q_\\pi(s,a) \\doteq E_\\pi[G_t\\ |\\ S_t=s,A_t=a] = E_\\pi\n",
    "\\begin{bmatrix}\n",
    "\\begin{equation}\n",
    "\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}\\mid S_t=s, A_t=a\n",
    "\\end{equation}\n",
    "\\end{bmatrix}. (3.13)\n",
    "$$\n",
    "<br>\n",
    "#### The value function $v_\\pi\\ and\\ q_\\pi$ can be estimated from experience.\n",
    "임의의 정책 $\\pi$를 따르는 agent 가 그 상태를 따르는 실제 반환값들의 평균을 유지한다면, 이를 무한대로 반복하면, 매 행동에 따라 분리된 평균 값들은 수렴하게 될 것이다. 우리는 이러한 추정 방법을 *Monte Carlo methods* 라고 부른다. 왜냐하면 많은 실제 반환 값들의 random samples 를 통해서 average 값을 획득하기 때문이다.\n",
    "##### 이러한 접근은 너무 많은 states 가 발생하므로 실용적이지 못 하여 파라메터 화 한  $v_\\pi\\ and\\ q_\\pi$ 함수를 사용한다.\n",
    "<br>\n",
    "#### Bellmann equation for $v_\\pi$\n",
    "$$ v_\\pi(s) \\doteq E_\\pi[G_t \\mid S_t=s]\\\\\n",
    "= \\begin{equation}\n",
    "\\sum_a \\pi(a|s)\n",
    "\\sum_{s',r} p(s',r \\mid s,a)[r+\\gamma v_\\pi(s')],\\ for\\ all\\ s \\in s,\n",
    "\\end{equation}\\ (3.14)\n",
    "$$\n",
    "> 벨만 방정식은 현재 상태 값과 계승자(successors)들의 상태 값들과의 관계를 나타낸다. 또한 시작 상태의 노드는 감가된(discounted) 다음 상태의 값과 일치해야만 한다.\n",
    "![backupdiagram](images/backupdiagram.png)\n",
    ":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Optimal Policies and Optimal Value Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Optimality and Approximation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
