{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 이 챕터에서는 finite markov decision processes 에 대해 소개한다. bandit problem과 마찬가지로 evaluative feedback 뿐만 아니라 associative aspect 또한 다루고 있다. (다른 situations에서 다른 actions들을 선택) \n",
    " \n",
    " **MDP(Markov Decision Process)**는 전형적인 순차적 의사 결정(sequential decision making)을 말하는데 행위(actions)들이 즉각적인 보상뿐만 아니라, 이후의 상황, 상태 그리고 미래의 보상(subsequent situations , or states, and through thoes future rewards)들에 영향을 준다. 그러므로 즉각적인 보상과 지연된 보상과의 균형(trade-off)을 다룬다.\n",
    " \n",
    " ### Approarch\n",
    "  bandit problem에서는 reward에 따른 action value function을 update하여 $q_*(a)$를 추정하는 문제였으나, MDP에서는 state s에 대한, action a를 선택하는 value function $q_*(s, a)$를 구하는 문제 혹은, 주어진 state에 대한 value function $v_*(a)$ 즉, 주어진 optimal action selection을 추정하는 문제로 바뀌었다. 이와 같이 상태 의존적인 정량적인 접근(state-dependent quantities)은 indivisual action selections 들에 대한 장기적인 결과(long-term consequences)에 좋은 영향을 준다. \n",
    "  \n",
    " 반환값(returns), 가치 함수(value functions), 벨만 방정식(Bellman equations) 과 같은 문제의 수학적인 구조의 핵심 요소에 대해서 설명한다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 The Agent-Environment Interface\n",
    "<br>\n",
    " #### *MDPs* ?\n",
    " 학습과 의사결정을 하는 *agent* 그리고 agent 이외의 모든 것을 의미하는 *environment* 이 두 객체의 interaction으로부터 goal 을 달성하기 위한 학습하는 문제를 다룬다.\n",
    " ![interaction.png](images/interaction.png)\n",
    "<center>**Figure 3.1: The agent-environment interaction in a Markov decision process.**</center>\n",
    " \n",
    "a *finite* MDP는 states, actions, and rewards (S, A, and R)들의 sets들이 finite number of elements 로 구성됨을 말하며, random variables $R_t$ and $S_t$ 는 바로 직전의 state, action 에만 종속적인 discrete probability ditributions 을 가진다.\n",
    "<br>\n",
    "#### *Markov property*?\n",
    " 확률변수 $S_t$ and $R_t$ 개별 값들의 확률이 바로 다음의 상태$S_{t-1}$와 액션$A_{t-1}$에만 영향을 주는 경우 Markov 속성을 가졌다고 말한다.\n",
    " $$ P[S_{t+1}\\ |\\ S_t] = P[S_{t+1}\\ |\\ S_1, ... S_t]$$\n",
    "##### The state captures all relevant information from the history\n",
    "##### Once the state is known, the history may be thrown away\n",
    "##### i.e. The state is a sufficient statistic of the future\n",
    "> 결국 markov property 를 만족할 수 있을 만큼 충부한 states 정보를 확보하는 것이 중요하다.\n",
    "<br> \n",
    "\n",
    "#### 1. four arguments *dynamics* function\n",
    "$$ p(s',r \\ | \\ s,a) \\doteq Pr \\{S_t=s',R_t=r\\ |\\ S_{t-1}=s,\\ A_{t-1}=a\\} , \\ (3.2)$$\n",
    "<center>MDP의 상태전이의 다양성을 나타낸 수식</center>\n",
    "$$ \\Sigma\\Sigma p(s',r \\ | \\ s,a)  = 1,\\ for\\ all\\ s \\in S, a \\in A(s). \\ (3.3) $$\n",
    "<center> 여기서 p는 모든 s, a 의 경우의 수에 따른 확률분포를 말한다.</center>\n",
    "<br>\n",
    "#### 2. *state-transition probabilities*\n",
    "$$ p(s'\\ |\\ s,a) \\doteq Pr \\{S_t=s'\\ |\\ S_{t-1}=s, A_{t-1}=a \\} = \n",
    "\\begin{equation}\n",
    "\\sum_{r\\in R}p(s',r\\ |\\ s,a)\n",
    "\\end{equation}\\ (3.4)\n",
    "$$\n",
    "<br>\n",
    "#### 3. two arguments *expected rewards for state-action pairs*\n",
    "$$ r(s,a) \\doteq E [R_t\\ |\\ S_{t-1}=s, A_{t-1}=a ] = \n",
    "\\begin{equation}\n",
    "\\sum_{r\\in R}\n",
    "\\sum_{s'\\in S}p(s',r\\ |\\ s,a)\n",
    "\\end{equation},\\  (3.5)\n",
    "$$\n",
    "<br>\n",
    "#### 4. three arguments *expected rewards for state-action-next-state triples*\n",
    "$$ r(s,a,s')\\doteq E [R_t\\ |\\ S_{t-1}=s,\\ A_{t-1}=a,\\ S_t\\ =\\ s'] = \n",
    "\\begin{equation}\n",
    "\\sum_{r\\in R}r \\frac{p(s',r\\ |\\ s,a)}{p(s'\\ |\\ s,a)}\n",
    "\\end{equation},\\  (3.6)\n",
    "$$\n",
    "<br>\n",
    "agent 와 environmetn 경계는 생각보다 agent 쪽에 가깝다. 로봇의 모터와 기계적인 연결 그리고 센싱 하드웨어 등은 모두 환경의 부품이라고 보여지면, 사람이나 동물의 경우에도 근육, 뼈대 및 인식기관 등도 환경의 부분이라고 간주된다. 보상들 또한 에이전트의 외부에서 전달된다, agent 가 environment 에 대해 항상 모르는 것이 아니라 Rubik's cube 맞추기와 같이 상황에 따라 전체 정보를 다 알고 있지만 해결하기 어려운 문제도 있다.\n",
    "\n",
    "##### * 결국, agent 와 environment 의 경계는 agent's 의 absolute control 의 한계이지 에이전트가 알고 있는 정보(knowledge)가 아니다.\n",
    "##### * states 와 actions 는 task 별로 아주 다양한데 어떻게 표현되는 지에 따라서 성능의 차이가 아주 크다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.3 Recycling Robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Goals and Rewards\n",
    "강화학습의 가장 큰 목표는 reward라고 불리는 scalar signal 값의 expected 누적 합의 최대화를 하는 방법을 배우는 것이다.\n",
    "##### * 목표의 아이디어를 형식화 하기 위한 보상 시그널을 사용하는 것이 강화학습의 가장 큰 특징 중의 하나이다.\n",
    "\n",
    "이러한 이해를 돕는 가장 좋은 방법은 실례를 들어보고 상상하는 것인데 깡통 줍는 로봇이 매 time step 마다 -1 보상을 받게 되고 깡통을 주었을 때에 +1 보상을 배터리가 방전되어 관리자가 옮기게 되는 경우 -3점 등으로 생각할 수 있다. 하지만 무엇 보다도 얻고자 하는 가장 중요한 목적을 전달하는 것이 중요하다.\n",
    "\n",
    "##### * 보상 신호(reward signal)은 agent 의 prior knowledge 를 담는 곳이 아니라 우리가 얻고자 하는 최종 목표를 알려주어야 하는데 체스의 예를 들면 승리하는 것이 목표가 되어야지 상대방의 좋은 말을 획득하는 것이 되어서는 안 된다.\n",
    "##### * The reward signal is your way of communicating to the robot what you want it to achieve, not how you want it achived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Returns and Episodes\n",
    "여태까지는 비형식적인 강화학습의 목표를 이야기 했다면 좀 더 형식적으로 표현하면 예상되는 반환값(expected return)이라 볼 수있다.\n",
    "$$ G_t\\ \\doteq R_{t+1}+R_{t+2}+R_{t+3}+\\ ... + R_T,\\ (3.7)$$\n",
    "<br>\n",
    "#### Differences between Goal & Task ?\n",
    "Goal 은 강화학습의 최종 목표이며, 체스에서 승리하거나 미로를 탈출하는 것이 그것이다\n",
    "Task 는 Goal \n",
    "#### Episodic Tasks\n",
    "*terminal state* 라고 불리우는 special state 에 도달하면 episode 가 종료되고, 시작 시점의 상태로 reset 된다.\n",
    "episode가 매번 다르게 종료된다고 하더라도, jk이전의 episode와 다음의 episode는 독립적이다\n",
    "동일한 종료 상태 가지더라도 다른 보상을 받을 수 있다\n",
    "때때로 종료 상태와 비 종료 상태의 구분 또는 종료 시간 T가 확률변수로 사용될 수 있다\n",
    "보상의 경우에 시간이 지남에 따른 감가(*discounting* )이 없다.\n",
    "#### Continuing Tasks\n",
    "정해진 종료 조건이 없고, 무한이 에피소드가 진행되는 경우\n",
    "final time step 이 $T=\\infty$, 이므로 최대화 할 수 없기 때문에 *discounting.* 기법을 적용하여 *discounted return*\n",
    "$$ G_t\\ \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\ ... =\n",
    "\\begin{equation}\n",
    "\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "\\end{equation},\\  (3.8)\n",
    "$$\n",
    "where $0 \\leq \\gamma \\leq 1$, called the *discount rate.*\n",
    "이는 미래의 보상을 현재에 계산하는 방식으로, 늦게 받으면 받을 수록 보상이 작아지는 효과를 가진다\n",
    "$\\gamma < 1$의 값을 가지면 (3.8) 식의 infinite sum 값은 $\\{R_k\\}$ 값에 바운드 되어 finite value를 가진다\n",
    "만약 $\\gamma = 0$이라면 \"myopic\" 즉 근시안 적인 눈 앞의 보상만 바라보는 결과를 가지게 되고,\n",
    "$\\gamma$ 값이 1에 가까워 질 수록 미래의 보상은 점점 커질 것이다.\n",
    "<br>\n",
    "#### reward is a constant +1, then return is\n",
    "$$ G_t = \n",
    "\\begin{equation}\n",
    "\\sum_{k=0}^\\infty \\gamma^k = \\frac{1}{1-\\gamma}.\n",
    "\\end{equation}\\ (3.9)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.4: Pole-Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Unified Notation for Episodic and Continuing Tasks\n",
    "Episodic Task 와 Continuing Task 를 하나의 수식으로 표현하고자 긴 task를 여러개의 episodic task로 생각할 수 있으며 $S_{t,i}$와 같이 i 번째 episode의 time t 를 의미하도록 표기할 수 있으나 때때로 생략할 수도 있다. 아래와 같이 episodic termination 구간을 두고 일정 구간 이후에는 *absorbing state * 상태로 빠지게 되고 보상이 0이 되는 것으로 표현이 가능하다.\n",
    "![absorbing_state](images/absorbingstate.png)\n",
    "#### absorbing state return is\n",
    "$$ G_t \\doteq\n",
    "\\begin{equation}\n",
    "\\sum_{k=t+1}^T \\gamma^{k-t-1}R_k,\n",
    "\\end{equation}\\ (3.11)\n",
    "$$\n",
    "위 식은 아래와 같이 풀어 쓸 수 있다\n",
    "$$G_t\\ \\doteq R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+\\ ... + \\gamma^{T-t-1}R_T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Policies and Value Functions\n",
    " 강화학습의 거의 모든 알고리즘에는 *value functions*을 추정하는 과정이 포함되어 있는데 이는 agent 가 *얼마나 좋은 상태로* 이끄는 가를 말하는 것인데 이는 예상할 수 있는 미래의 가치들을 의미하는데 이 가치함수를 만들어내는 데에는 소위 정책(policies)이라고 하는 행동 하는 방법에 대한 가치함수를 말한다.\n",
    "<br>\n",
    "#### **정책** or Policy?\n",
    "##### 상태로부터 선택 가능한 행동들에 이르는 확률\n",
    "##### $\\pi(a\\mid s)$ : $A_t=a\\ if\\ S_t=s$. 즉 상태 s 일때 행동 a를 수행할 확률.\n",
    "##### 강화학습의 방법들은 경험의 결과에 의하여 어떻게 에이전트의 정책이 변경 되는 지에 대한 것이라 말할 수 있다.\n",
    "<br>\n",
    "#### state-value function for policy $\\pi$\n",
    "$$ v_\\pi(s) \\doteq E_\\pi[G_t\\ |\\ S_t=s] = E_\\pi\n",
    "\\begin{bmatrix}\n",
    "\\begin{equation}\n",
    "\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}\\mid S_t=s\n",
    "\\end{equation}\n",
    "\\end{bmatrix}\n",
    ",\\ for \\ all \\ s \\in S,\\ (3.12)\n",
    "$$\n",
    "<br>\n",
    "$v_\\pi(s)$ 는 *state-value function for policy $\\pi$.\n",
    "<br>\n",
    ">**상태-가치 함수** 는 <font color=orange>정책함수 $\\pi$를 따르는 agent 의 보상 확률변수 R에 대한 expected value </font>\n",
    "\n",
    "#### action-value function for policy $\\pi$.\n",
    "$$ q_\\pi(s,a) \\doteq E_\\pi[G_t\\ |\\ S_t=s,A_t=a] = E_\\pi\n",
    "\\begin{bmatrix}\n",
    "\\begin{equation}\n",
    "\\sum_{k=0}^\\infty \\gamma^kR_{t+k+1}\\mid S_t=s, A_t=a\n",
    "\\end{equation}\n",
    "\\end{bmatrix}. (3.13)\n",
    "$$\n",
    "<br>\n",
    ">**행동-가치 함수** 는 <font color=orange>정책함수 $\\pi$를 따르는 agent 의 action에 대한 보상 확률변수 R에 대한 expected value </font>\n",
    "\n",
    "#### The value function $v_\\pi\\ and\\ q_\\pi$ can be estimated from experience.\n",
    " 임의의 정책 $\\pi$를 따르는 agent 가 그 상태를 따르는 실제 반환값들의 평균을 반복해서 계산하게 된다면, 그리고 이를 무한대로 반복하였을 때에, 매 행동에 따라 분리된 평균 값들은 수렴하게 될 것이다. 우리는 이러한 추정 방법을 *Monte Carlo methods* 라고 부른다. 왜냐하면 많은 실제 반환 값들의 random samples 를 통해서 average 값을 획득하기 때문이다.\n",
    "##### 이러한 접근은 너무 많은 states 가 발생하므로 실용적이지 못 하여 <font color=red>파라메터 화 한  $v_\\pi\\ and\\ q_\\pi$ 함수the parameterized function approximator를 사용할 수 있다.</font>\n",
    "<br>\n",
    "#### Bellmann equation for $v_\\pi$\n",
    "$$ v_\\pi(s) \\doteq E_\\pi[G_t \\mid S_t=s]\\\\\n",
    "= \\begin{equation}\n",
    "\\sum_a \\pi(a\\mid s)\n",
    "\\sum_{s',r} p(s',r \\mid s,a)[r+\\gamma v_\\pi(s')],\\ for\\ all\\ s \\in s,\n",
    "\\end{equation}\\ (3.14)\n",
    "$$\n",
    "> 벨만 방정식은 현재 상태 값과 계승자(successors)들의 상태 값들과의 관계를 나타낸다. 또한 시작 상태의 노드는 감가된(discounted) 다음 상태의 값과 일치해야만 한다.\n",
    "\n",
    "![backupdiagram](images/backupdiagram.png)\n",
    "##### 초기 상태 $S$로부터 agent는 정책함수 $\\pi$를 통해 3가지의 action을 선택할 수 있다.\n",
    "##### environment는 dynamics function $p$에 따라 $S'$으로 전환되고 보상 `$r$을 받는다.\n",
    "##### 위와 같은 diagram을 backup diagrams라고 한다\n",
    "##### 아마 자식 노드에서 발생하는 reward값이 root node로 지속적으로 backup 되는 것처럼 보이기 때문인 것 같다.\n",
    "##### 이러한 backup operations 이 reinforcement learning의 중요한 위치를 차지하고 있다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3.5: GridWorld\n",
    "Simple Finite MDP 구현 예제\n",
    "##### 5x5 격자 사각형 내에서 north, south, east and west 4방향으로 무조건 이동이 가능하고 보상은 없고,\n",
    "##### 단, 격자 밖으로 이동 시에는 그 자리에 유지되지만 -1 보상이 주어지고,\n",
    "##### 특정 위치 A, B 에서는 무조건 A', B' 로 이동하며 각 각 +10, +5 보상이 주어진다.\n",
    "<br>\n",
    "#### Lessons from mistakes\n",
    "1. 수렴하지 않고 하나의 상태만 계속 나오는 문제: state = next_state 할당 이후에 model update 한 오류\n",
    "1. (3,0) 값이 5점 이상이 나와야 하는데 4점 이하로 나오는 문제: reward 계산 시에 +10, +5 점 보다 -1 점 제약을 먼저 해서 오류 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding:utf-8 -*-\n",
    "import numpy as np\n",
    "import sys\n",
    "import copy\n",
    "\n",
    "debug_flag = False\n",
    "\n",
    "def write(args):\n",
    "    for arg in args:\n",
    "        print(arg, end=' ', flush=True)\n",
    "    print()\n",
    "\n",
    "def debug(*args):\n",
    "    if (debug_flag):\n",
    "        write(args)\n",
    "        \n",
    "def info(*args):\n",
    "    write(args)\n",
    "    \n",
    "class Bar:\n",
    "    def __str__(self):\n",
    "        return \"({}, {})\".format(\"a\", \"b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "(<__main__.Bar object at 0x1137cdf60>, <__main__.Bar object at 0x1137cdf60>, <__main__.Bar object at 0x1137cdf60>)\n",
      "<class 'tuple'>\n",
      "(<__main__.Bar object at 0x1137cdf60>,)\n"
     ]
    }
   ],
   "source": [
    "def foo():\n",
    "    width=5\n",
    "    height=5\n",
    "    x = [[0.00] * width for _ in range(height)]\n",
    "    x[0][1] = +10\n",
    "    x[0][3] = +5\n",
    "    print(x)\n",
    "    \n",
    "def a():\n",
    "    print(\"a\")\n",
    "    \n",
    "def b():\n",
    "    print(\"b\")\n",
    "    \n",
    "def c():\n",
    "    print(\"c\")\n",
    "    \n",
    "def d():\n",
    "    print(\"d\")\n",
    "    \n",
    "def random_function():\n",
    "    functions=[a, b, c, d]\n",
    "    x = np.random.randint(0, 4, 1)[0]\n",
    "    f = functions[x]\n",
    "    f()\n",
    "    \n",
    "def bar():\n",
    "    x = 4\n",
    "    x -= 2\n",
    "    print(x)\n",
    "    \n",
    "b = Bar()\n",
    "debug(b, b, b)\n",
    "debug(b)\n",
    "print(\"OK\")\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class State:\n",
    "    \n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def x(self):\n",
    "        return x\n",
    "    \n",
    "    def y(self):\n",
    "        return y\n",
    "    \n",
    "    def equals(self, state):\n",
    "        return self.x == state.x and self.y == state.y\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"(x:{}, y:{})\".format(self.x, self.y)\n",
    "\n",
    "class Action:\n",
    "    \n",
    "    def __init__(self, state):\n",
    "        self.state = copy.deepcopy(state)\n",
    "        self.functions = [self.north, self.south, self.east, self.west]\n",
    "        self.num_of_actions = len(self.functions)\n",
    "        \n",
    "    def north(self):\n",
    "        self.state.y -= 1\n",
    "        return self.state\n",
    "    \n",
    "    def south(self):\n",
    "        self.state.y += 1\n",
    "        return self.state\n",
    "    \n",
    "    def east(self):\n",
    "        self.state.x += 1\n",
    "        return self.state\n",
    "    \n",
    "    def west(self):\n",
    "        self.state.x -= 1\n",
    "        return self.state\n",
    "    \n",
    "    def random(self):\n",
    "        x = np.random.randint(0, 4, 1)[0]\n",
    "        return self.functions[x]\n",
    "    \n",
    "    def actionOf(self, x):\n",
    "        return self.functions[x]\n",
    "\n",
    "    def getNumOfActions(self):\n",
    "        return self.num_of_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "[0.25, 0.25, 0.25, 0.25]\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "s = State(0, 4)\n",
    "a = Action(s)\n",
    "print(a.getNumOfActions())\n",
    "action = a.random()\n",
    "sp = action()\n",
    "debug(s, \"->\", sp)\n",
    "# debug(prev_state, \"->\", self.state)\n",
    "num_of_actions=4\n",
    "policy_table = [1.0/num_of_actions] * num_of_actions\n",
    "print(policy_table)\n",
    "policy_factor = 1.0/num_of_actions\n",
    "print(policy_factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \n",
    "    def __init__(self, width=5, height=5):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.reward = [[0] * width for _ in range(height)]\n",
    "        self.a = State(1, 0)\n",
    "        self.next_a = State(1, 4)\n",
    "        self.b = State(3, 0)\n",
    "        self.next_b = State(3, 2)\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "    # out-of-bound returns -1 reward\n",
    "    # (0,1) = 10, (0,3) = 5  rewards, else 0 reward\n",
    "    def step(self, state, action, debug_print=False):\n",
    "        next_state = action()\n",
    "        _state, _reward = None, None\n",
    "        if self.a.equals(state):\n",
    "            debug('Found 10 points')\n",
    "            _state, _reward = self.next_a, +10\n",
    "        elif self.b.equals(state):\n",
    "            debug('Found +5 points')\n",
    "            _state, _reward = self.next_b, +5\n",
    "        elif next_state.x >= width or next_state.y >= height or next_state.x < 0 or next_state.y < 0:\n",
    "            _state, _reward = state, -1\n",
    "            debug('Found -1 point')\n",
    "        else:\n",
    "            _state, _reward = next_state, 0\n",
    "        if debug_print:\n",
    "            info('state:', state, '=>', _state, ', reward:', _reward)\n",
    "        return _state, _reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.29, 8.78, 4.42, 5.31, 1.48]\n",
      "[1.5, 2.98, 2.24, 1.89, 0.53]\n",
      "[0.03, 0.72, 0.66, 0.34, -0.42]\n",
      "[-0.99, -0.45, -0.37, -0.6, -1.2]\n",
      "[-1.87, -1.36, -1.24, -1.44, -1.99]\n"
     ]
    }
   ],
   "source": [
    "class GridWorld:\n",
    "    \n",
    "    def __init__(self, env, discount_factor=0.9, value_table=None):\n",
    "        self.env = env\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_of_actions = Action(State(0, 0)).getNumOfActions()\n",
    "        self.policy_factor = 1.0/self.num_of_actions\n",
    "        self.value_table = [[0.0] * env.width for _ in range(env.height)]\n",
    "        if value_table != None:\n",
    "            self.value_table = value_table\n",
    "        \n",
    "    def getAction(self, state, x):\n",
    "        return Action(state).actionOf(x)\n",
    "    \n",
    "    def updateModel(self, state):\n",
    "        _reward = 0.0\n",
    "        for x in range(self.num_of_actions):\n",
    "            action = self.getAction(state, x)\n",
    "            next_state, reward = self.env.step(state, action, False) # sigma p(s',r|s,a)\n",
    "            _reward += reward + self.discount_factor * self.value_table[next_state.y][next_state.x]\n",
    "        return round((self.policy_factor * _reward), 2)\n",
    "        \n",
    "    def getRandomState(self):\n",
    "        pass\n",
    "        \n",
    "    # implementation of Bellmann equation (3.14)\n",
    "    def start(self, num_of_episodes = 1000, num_of_steps = 1000):\n",
    "        env = self.env\n",
    "        env.reset()\n",
    "        for episode in range(num_of_episodes):\n",
    "            state = State(3, 0) # start with center position\n",
    "            for step in range(num_of_steps):\n",
    "                action = Action(state)\n",
    "                next_state, reward = env.step(state, action.random(), False) # sigma p(s',r|s,a)\n",
    "                total_reward = self.updateModel(state) # expectation of [r + gamma v(s')]\n",
    "                self.value_table[state.y][state.x] = total_reward\n",
    "                state = next_state\n",
    "        \n",
    "    def printStateValue(self):\n",
    "        for x in range(self.env.width):\n",
    "            print(self.value_table[x])\n",
    "                \n",
    "    def printStateValueRev(self):\n",
    "        v = self.value_table\n",
    "        for y in range(self.env.height):\n",
    "            for x in range(self.env.width):\n",
    "                print(v[x][y], end=' ')\n",
    "            print()\n",
    "\n",
    "# 디버깅을  위해 초깃값을 기존에 1000 step 수행한 결과를 지정할 수 있도록 수정\n",
    "def getValueTable():\n",
    "    value_table=[\n",
    "        [2.74, 8.05, 3.79, 4.56, 0.21],\n",
    "        [1.08, 2.54, 1.79, 1.41, -0.09],\n",
    "        [-0.21, 0.47, 0.4, 0.06, -0.74],\n",
    "        [-1.15, -0.61, -0.53, -0.77, -1.38],\n",
    "        [-2.0, -1.48, -1.36, -1.56, -2.12]]\n",
    "    return value_table\n",
    "    \n",
    "def main(num_of_episodes = 1000, num_of_steps = 1000):\n",
    "    initial_value_table = getValueTable()\n",
    "    env = Environment()\n",
    "    gridWorld = GridWorld(env, value_table=initial_value_table)\n",
    "    gridWorld.start(num_of_episodes, num_of_steps)\n",
    "    gridWorld.printStateValue()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(100, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Optimal Policies and Optimal Value Functions\n",
    " 강화학습 문제를 푸는 작업은 긴 기간에 걸처서 최대한 많은 보상을 얻는 정책을 찾는 것인데, finite MDP의 optimal policy를 아래와 같이 정의할 수 있다\n",
    " ##### 모든 상태에 대한 정책  $\\pi$를 어떠한 $\\pi'$ 보다 나은 정책이라면 $v_\\pi(s) \\leq v_\\pi'(s)\\ for\\ all\\ s \\in S$\n",
    " ##### 단 하나의 최적의 정책 $\\pi_*$이 존재하며, 동일한 상태-가치 함수를 공유하는 **optimal state-value function** $v_*$\n",
    "$$v_*(s) \\doteq max_\\pi v_\\pi(s), (3.15)$$\n",
    "$\\ for\\ all\\ s\\in S.$\n",
    "<br> \n",
    " ##### optimal policies 또한 동일한 **optimal action-value function**을 공유하는데 $q_*$\n",
    " $$q_*(s,a)\\doteq max_\\pi q_\\pi(s,a),\\ (3.16)$$\n",
    " $\\ for\\ all\\ s \\in S\\ and\\ a \\in A(s).$\n",
    " <br>\n",
    " ##### 최적의 정책을 따르는, 상태s 에서 행동a를 수행할 때에 q함수는 아래와 같다.\n",
    " $$q_*(s,a)=E[R_{t+1}+\\gamma v_*(S_{t+1} \\mid S_t=s,A_t=a].\\ (3.17)$$\n",
    " \n",
    "#### Bellmann <font color=red>optimal equation for $q_*$</font>\n",
    "$$ v_*(s) = E[R_{t+1} + \\gamma max_{a'}q_*(S_{t+1}\\mid S_t=s,A_t=a]\\\\\n",
    "= \\begin{equation}\n",
    "\\sum_{s',r} p(s',r \\mid s,a)[r+\\gamma max_{a'}\\ q_*(s',a')],\n",
    "\\end{equation}\\ (3.20)\n",
    "$$\n",
    "> Bellmann equation 식과 다른 점은 정책함수 $\\pi$ 항이 존재하지 않는데, 이는 최적의 행동action 을 이미 알고 있기 때문에 굳이 계산할 필요가 없기 때문이다.\n",
    "\n",
    "##### environment에서 dynamics p 함수를 아는 경우 unique solution 즉 최적 방정식을 알 수 있다\n",
    "##### 이는 state, reward 를 모두 알 수 있다는 의미이며, state-value $v_*$, action-value $q_*$ 를 알 수 있다.\n",
    "##### 일단 $v_*$를 안다는 말은 모든 상황state에서 최적의 행동action을 할 수 있다는 의미이다.\n",
    "##### 심지어 greedy한 접근을 하여도 optimal state에 이르는데 이는 모든 가능한 behavior에 대해 알고 있기 때문이다.\n",
    "> 결국 action-value function을 알지 못해도 state-value function 만으로도 one-step-ahead lookup 을 통해 optimal 선택을 할 수 있다. 하지만 action-value function을 알고 있다면 one-step lookup 만으로 최적의 선택을 할 수 있다. 이는 Example 3.5를 통해서 state-value function 만으로도 최적의 상태를 알 수 있다는 것을 확인하였다\n",
    "\n",
    "#### Bellman optimal equation 문제는 아래의 조건을 만족한다면 유용하다\n",
    "1. dynamics of environment 정보를 아주 정확히 알고 있다\n",
    "1. computatinal resources 가 충분히 있다\n",
    "1. Markov property 를 만족한다\n",
    "\n",
    "> 하지만 그리 복잡하지 않은 'backgammon' 프로그램의 경우만 하더라도 $10^{20}$의 state를 가지기 때문에 모든 상태를 계산하기에는 현 세대 최고의 컴퓨터로도 수 천년이 걸릴 수도 있을 것이다. 그래서 강화학습 영역에서는 approximation solutions 으로 접근한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Optimality and Approximation\n",
    "### Optimal policy의 한계점\n",
    "1. extreme computational cost\n",
    "1. dynamics of environment\n",
    "1. curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Summary\n",
    "<br>\n",
    "### '강화 학습' 이란?\n",
    "> 목표goal를 달성하기 위하여 상호작용interaction을 통하여 어떻게 행동하는 지를 배우는 것이다.\n",
    "\n",
    "### '강화 학습' 관련 용어들\n",
    " 1. Agent : 환경과 상호작용하는 학습의 주체\n",
    " 1. Environment : 에이전트의 행위 이외의 모든 상태\n",
    " 1. Action : 에이전트가 환경의 상태에 반응하여 행동하는 동작\n",
    " 1. State : 에이전트가 처한 현재 상태\n",
    " 1. Reward : 에이전트의 행위에 따른 환경의 보상\n",
    " 1. Policy : 환경의 상태에 따라 에이전트가 어떤 행동을 하는 지를 결정하는 정책\n",
    " 1. MDP : 위의 1~6까지의 요소들을 통해서 순차적 행동 결정 문제를 수학적으로 표현한 것\n",
    " 1. Finite MDP : MDP 문제에서 State, Action 그리고 Reward 등의 집합이 finite set 인 경우\n",
    " 1. Return : 반환값을 말하는데 즉각적인 보상이 아니라 긴 기간 동안에 감가율이 적용된 보상의 미래 예측치\n",
    " 1. Episodic Task : terminate state 가 존재하는 환경\n",
    " 1. Continuing Task : terminate state 가 존재하지 않고 끝없이 계속 되는 환경\n",
    " 1. Bellman Equation : \n",
    " 1. Bellman Optimality Equations : \n",
    " 1. Complete Knowledge : 에이전트가 환경의 모델에 대한 모든 정보를 다 알고 있는 경우\n",
    " 1. Incomplete Knowledge : 에이전트가 환경의 모델에 대해 일부만 아는 경우\n",
    " \n",
    "> 우리는 이러한 최적의 솔루션을 발견하기 어렵기 때문에 approximation 하는 방법들을 찾아야만 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
